{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec87467-76f0-4fe3-b498-8783d761e085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading page...\n",
      "\n",
      "Page 1/5\n",
      "  Found: 60 | New: 60\n",
      "  Total collected: 60\n",
      "\n",
      "Page 2/5\n",
      "  Found: 60 | New: 60\n",
      "  Total collected: 120\n",
      "\n",
      "Page 3/5\n",
      "  Found: 60 | New: 60\n",
      "  Total collected: 180\n",
      "\n",
      "Page 4/5\n",
      "  Found: 60 | New: 60\n",
      "  Total collected: 240\n",
      "\n",
      "Page 5/5\n",
      "  Found: 60 | New: 60\n",
      "  Total collected: 300\n",
      "\n",
      "==================================================\n",
      "Total listings collected: 300\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Scrape details for 300 listings? (y/n):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping details...\n",
      "[10/300] 0.8m | 12.1/min | ~23.9m left\n",
      "[20/300] 1.6m | 12.2/min | ~22.9m left\n",
      "[30/300] 2.4m | 12.7/min | ~21.3m left\n",
      "[40/300] 3.3m | 12.2/min | ~21.3m left\n",
      "[50/300] 4.0m | 12.4/min | ~20.1m left\n",
      "[60/300] 4.8m | 12.5/min | ~19.2m left\n",
      "[70/300] 5.7m | 12.4/min | ~18.6m left\n",
      "[80/300] 6.5m | 12.4/min | ~17.7m left\n",
      "[90/300] 7.2m | 12.5/min | ~16.8m left\n",
      "[100/300] 7.9m | 12.6/min | ~15.8m left\n",
      "[110/300] 8.7m | 12.7/min | ~15.0m left\n",
      "[120/300] 9.4m | 12.8/min | ~14.1m left\n",
      "[130/300] 10.1m | 12.8/min | ~13.2m left\n",
      "[140/300] 10.9m | 12.9/min | ~12.4m left\n",
      "[150/300] 11.7m | 12.9/min | ~11.7m left\n",
      "[160/300] 12.4m | 12.9/min | ~10.9m left\n",
      "[170/300] 13.1m | 12.9/min | ~10.0m left\n",
      "[180/300] 13.9m | 13.0/min | ~9.2m left\n",
      "[190/300] 14.6m | 13.0/min | ~8.5m left\n",
      "[200/300] 15.3m | 13.0/min | ~7.7m left\n",
      "[210/300] 16.2m | 13.0/min | ~6.9m left\n",
      "[220/300] 17.0m | 12.9/min | ~6.2m left\n",
      "[230/300] 17.8m | 12.9/min | ~5.4m left\n",
      "[240/300] 18.6m | 12.9/min | ~4.6m left\n",
      "[250/300] 19.3m | 13.0/min | ~3.9m left\n",
      "[260/300] 20.0m | 13.0/min | ~3.1m left\n",
      "[270/300] 20.8m | 13.0/min | ~2.3m left\n",
      "[280/300] 21.5m | 13.0/min | ~1.5m left\n",
      "[290/300] 22.2m | 13.0/min | ~0.8m left\n",
      "[300/300] 23.0m | 13.0/min | ~0.0m left\n",
      "\n",
      "==================================================\n",
      "Done! Scraped 300/300 listings\n",
      "Saved to: Unegui_bayanzurkh_20251112.csv\n",
      "==================================================\n",
      "\n",
      "     –®–∞–ª:      –¢–∞–≥—Ç:   –ì–∞—Ä–∞–∂:       –¶–æ–Ω—Ö: –•–∞–∞–ª–≥–∞: –¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:      –ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü  \\\n",
      "0  –ü–∞—Ä–∫–µ—Ç   1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π       –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          5  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "1  –ü–∞—Ä–∫–µ—Ç  3+—Ç–∞–≥—Ç—Ç–∞–π   –ë–∞–π–≥–∞–∞       –í–∞–∫—É–º   –¢”©–º”©—Ä          8  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "2  –ü–∞—Ä–∫–µ—Ç   1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π       –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          6  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "3  –ü–∞—Ä–∫–µ—Ç   1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π       –í–∞–∫—É–º   –¢”©–º”©—Ä          3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "4  –ü–∞—Ä–∫–µ—Ç   1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –¢”©–º”©—Ä–≤–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          8  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "\n",
      "  –ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω: –ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:  –¢–∞–ª–±–∞–π:  ... –î“Ø“Ø—Ä—ç–≥:  \\\n",
      "0                2008              12  103.0–º¬≤  ...      –£–ë   \n",
      "1                2007              13  356.0–º¬≤  ...      –£–ë   \n",
      "2                2018              16  134.0–º¬≤  ...      –£–ë   \n",
      "3                2025              16  81.03–º¬≤  ...      –£–ë   \n",
      "4                2021              16  64.29–º¬≤  ...      –£–ë   \n",
      "\n",
      "                       –ë–∞–π—Ä—à–∏–ª:       “Æ–∑—Å—ç–Ω: Scraped_date:  \\\n",
      "0   –ë–∞—è–Ω–∑“Ø—Ä—Ö, –ë–∞—è–Ω–∑“Ø—Ä—Ö, –•–æ—Ä–æ–æ 6    “Æ–∑—Å—ç–Ω : 2    2025/11/12   \n",
      "1   –ë–∞—è–Ω–∑“Ø—Ä—Ö, –ë–∞—è–Ω–∑“Ø—Ä—Ö, –•–æ—Ä–æ–æ 1   “Æ–∑—Å—ç–Ω : 33    2025/11/12   \n",
      "2  –ë–∞—è–Ω–∑“Ø—Ä—Ö, –ë–∞—è–Ω–∑“Ø—Ä—Ö, –•–æ—Ä–æ–æ 26  “Æ–∑—Å—ç–Ω : 198    2025/11/12   \n",
      "3  –ë–∞—è–Ω–∑“Ø—Ä—Ö, –ë–∞—è–Ω–∑“Ø—Ä—Ö, –•–æ—Ä–æ–æ 37    “Æ–∑—Å—ç–Ω : 5    2025/11/12   \n",
      "4   –ë–∞—è–Ω–∑“Ø—Ä—Ö, –ë–∞—è–Ω–∑“Ø—Ä—Ö, –•–æ—Ä–æ–æ 3   “Æ–∑—Å—ç–Ω : 11    2025/11/12   \n",
      "\n",
      "                  Posted_date:  \\\n",
      "0     –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 15:24   \n",
      "1     –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 10:48   \n",
      "2  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-11-07 09:23   \n",
      "3     –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 22:23   \n",
      "4     –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 17:05   \n",
      "\n",
      "                                               Link:           “Æ–Ω—ç:  \\\n",
      "0  https://www.unegui.mn/adv/9649216_naturt-3-oro...   365000000.00   \n",
      "1  https://www.unegui.mn/adv/9890552_bzd-1-r-khor...  1500000000.00   \n",
      "2  https://www.unegui.mn/adv/9814153_bzd-26-r-kho...   660000000.00   \n",
      "3  https://www.unegui.mn/adv/9870714_chanartai-zo...     3550000.00   \n",
      "4  https://www.unegui.mn/adv/9893370_tokio-reside...   392169000.00   \n",
      "\n",
      "  ”®—Ä”©”©–Ω–∏–π –¢–æ–æ:                                      –ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:  \\\n",
      "0       3 ”©—Ä”©”©                           –ù–∞—Ç—É—Ä—Ç 3 ”©—Ä”©”© 103–º2 –±–∞–π—Ä   \n",
      "1      5+ ”©—Ä”©”©         –ë–∑–¥ 1-—Ä —Ö–æ—Ä–æ–æ royal castel 10 ”©—Ä”©”© –¥“Ø–ø–ª–µ–∫—Å   \n",
      "2       4 ”©—Ä”©”©  –ë–∑–¥, 26-—Ä —Ö–æ—Ä–æ–æ emerald residence 4 ”©—Ä”©”© –±–∞–π—Ä ...   \n",
      "3       3 ”©—Ä”©”©                            –ë–∑–¥ 3 ”©—Ä”©”© –±–∞–π—Ä 81.03–º2   \n",
      "4       2 ”©—Ä”©”©                   –¢–æ–∫–∏–æ —Ä–µ—Å–∏–¥–µ–Ω—Å—Ç 64,29 –º–∫–≤ 2 ”©—Ä”©”©   \n",
      "\n",
      "                                      –ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:  \n",
      "0                         –ë“Ø—Ö —é–º–∞–Ω–¥–∞–∞ –æ–π—Ä —Ö–æ—Ç—ã–Ω —Ç”©–≤–¥  \n",
      "1  –ë–ó–î ùóò-ùó†ùóÆùóøùòÅ-–Ω —Ö–æ–∏ÃÜ–Ω–æ ùó•ùóºùòÜùóÆùóπ ùóñùóÆùòÄùòÅùóπùó≤ ùü≠ùü¨ ”©—Ä”©”© –¥“Ø–ø–ª–µ...  \n",
      "2  ‚òÄÔ∏è–ë–ó–î , 26-—Ä —Ö–æ—Ä–æ–æ Emerald Residence —Ö–æ—Ç—Ö–æ–Ω–¥ 1...  \n",
      "3  –ß–∞–Ω–∞—Ä—Ç–∞–π, –∑”©–≤ –∑–æ—Ö–∏–æ–Ω –±–∞–π–≥—É—É–ª–∞–ª—Ç—Ç–∞–π\\n\\n–ñ–∏–≥“Ø“Ø—Ä –≥...  \n",
      "4  –ë”©—Ö–∏–π–Ω ”©—Ä–≥”©”©–Ω–∏–π –∑“Ø“Ø–Ω —Ö–æ–π–Ω–æ, —Ç–æ–∫–∏–æ —Ä–µ—Å–∏–¥–µ–Ω—Å –¥ 6...  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "header = ['–®–∞–ª:', '–¢–∞–≥—Ç:', '–ì–∞—Ä–∞–∂:', '–¶–æ–Ω—Ö:', '–•–∞–∞–ª–≥–∞:', '–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', '–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', \n",
    "          '–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', '–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', '–¢–∞–ª–±–∞–π:', '–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', \n",
    "          '–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', '–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', '–î“Ø“Ø—Ä—ç–≥:', '–ë–∞–π—Ä—à–∏–ª:', '“Æ–∑—Å—ç–Ω:', \n",
    "          'Scraped_date:', 'Posted_date:', 'Link:', '“Æ–Ω—ç:', '”®—Ä”©”©–Ω–∏–π –¢–æ–æ:', \n",
    "          '–ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:', '–ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:']\n",
    "\n",
    "csv_list = []\n",
    "\n",
    "def key_finder(key, index, span_list, key_list):\n",
    "    for item in span_list:\n",
    "        if str(key) in item:\n",
    "            key_list[index] = item.split(':')[1]\n",
    "    if type(key_list[int(index)]) != str:\n",
    "        key_list[int(index)] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "def extract_listing_links(soup):\n",
    "    listing_links = set()\n",
    "    pattern = re.compile(r'^/adv/\\d+_[a-z0-9-]+/?$')\n",
    "    \n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        if pattern.match(href):\n",
    "            if not any(exclude in href for exclude in ['?', '#', 'page=', 'sort=', 'view=']):\n",
    "                listing_links.add(href)\n",
    "    \n",
    "    return listing_links\n",
    "\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = uc.Chrome(options=options, version_main=None) \n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "try:\n",
    "    base_url = 'https://www.unegui.mn/l-hdlh/l-hdlh-zarna/oron-suuts-zarna/ub-bayanzrh/'   \n",
    "    \n",
    "    print(\"Loading page...\")\n",
    "    driver.get(base_url)\n",
    "    time.sleep(5)  \n",
    "    \n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    all_listing_urls = set()\n",
    "    current_page = 1\n",
    "    max_pages = 5  # Reduced to 5 for faster testing\n",
    "    no_new_count = 0\n",
    "    \n",
    "    while current_page <= max_pages:\n",
    "        print(f\"\\nPage {current_page}/{max_pages}\")\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        page_listings = extract_listing_links(soup)\n",
    "        \n",
    "        # ‚úÖ FIX: Actually check and add new listings!\n",
    "        new_listings = page_listings - all_listing_urls\n",
    "        print(f\"  Found: {len(page_listings)} | New: {len(new_listings)}\")\n",
    "        \n",
    "        if len(new_listings) == 0:\n",
    "            no_new_count += 1\n",
    "            if no_new_count >= 2:\n",
    "                print(\"No more new listings, stopping\")\n",
    "                break\n",
    "        else:\n",
    "            no_new_count = 0\n",
    "            all_listing_urls.update(new_listings)\n",
    "            print(f\"  Total collected: {len(all_listing_urls)}\")\n",
    "        \n",
    "        # Navigate to next page\n",
    "        if current_page < max_pages:\n",
    "            next_page_url = f\"{base_url}?page={current_page + 1}\"\n",
    "            driver.get(next_page_url)\n",
    "            time.sleep(4)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            current_page += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    listing_urls = [f\"https://www.unegui.mn{link}\" for link in all_listing_urls]\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Total listings collected: {len(listing_urls)}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    if len(listing_urls) == 0:\n",
    "        print(\"No listings found\")\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    \n",
    "    response = input(f\"\\nScrape details for {len(listing_urls)} listings? (y/n): \")\n",
    "    if response.lower() != 'y':\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    \n",
    "    print(\"\\nScraping details...\")\n",
    "    start_time = time.time()\n",
    "    successful = 0\n",
    "    \n",
    "    for i, listing_url in enumerate(listing_urls, 1):\n",
    "        try:\n",
    "            if i % 10 == 0:\n",
    "                elapsed = (time.time() - start_time) / 60\n",
    "                rate = i / (time.time() - start_time) * 60\n",
    "                remaining = (len(listing_urls) - i) / rate if rate > 0 else 0\n",
    "                print(f\"[{i}/{len(listing_urls)}] {elapsed:.1f}m | {rate:.1f}/min | ~{remaining:.1f}m left\")\n",
    "            \n",
    "            driver.get(listing_url)\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            key_list = [\"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"] * len(header)\n",
    "            \n",
    "            try:\n",
    "                details = soup.find('div', class_='announcement-characteristics clearfix').find_all('li')\n",
    "                span_list = [item.text.replace('\\n', '').replace(' ', '') for item in details]\n",
    "            except:\n",
    "                span_list = []\n",
    "            \n",
    "            key_finder('–®–∞–ª:', 0, span_list, key_list)\n",
    "            key_finder('–¢–∞–≥—Ç:', 1, span_list, key_list)\n",
    "            key_finder('–ì–∞—Ä–∞–∂:', 2, span_list, key_list)\n",
    "            key_finder('–¶–æ–Ω—Ö:', 3, span_list, key_list)\n",
    "            key_finder('–•–∞–∞–ª–≥–∞:', 4, span_list, key_list)\n",
    "            key_finder('–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', 5, span_list, key_list)\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', 6, span_list, key_list)\n",
    "            key_finder('–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', 7, span_list, key_list)\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', 8, span_list, key_list)\n",
    "            key_finder('–¢–∞–ª–±–∞–π:', 9, span_list, key_list)\n",
    "            key_finder('–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', 10, span_list, key_list)\n",
    "            key_finder('–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', 11, span_list, key_list)\n",
    "            key_finder('–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', 12, span_list, key_list)\n",
    "            \n",
    "            try:\n",
    "                address = soup.find('span', itemprop=\"address\").text.split('‚Äî')\n",
    "                key_list[13] = address[0].strip() if len(address) > 0 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "                key_list[14] = address[1].strip() if len(address) > 1 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            except:\n",
    "                key_list[13] = key_list[14] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            try:\n",
    "                key_list[15] = soup.find('span', class_='counter-views').text.strip()\n",
    "            except:\n",
    "                key_list[15] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            key_list[16] = date.today().strftime(\"%Y/%m/%d\")\n",
    "            \n",
    "            try:\n",
    "                key_list[17] = soup.find('span', class_='date-meta').text.strip()\n",
    "            except:\n",
    "                key_list[17] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            key_list[18] = listing_url\n",
    "            \n",
    "            try:\n",
    "                key_list[19] = soup.find('meta', itemprop='price')['content']\n",
    "            except:\n",
    "                key_list[19] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            try:\n",
    "                key_list[20] = soup.find('div', class_='wrap js-single-item__location').find_all('span')[-1].text.strip()\n",
    "            except:\n",
    "                key_list[20] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            try:\n",
    "                key_list[21] = soup.find('h1', class_='title-announcement').text.strip()\n",
    "            except:\n",
    "                key_list[21] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            try:\n",
    "                key_list[22] = soup.find('div', class_='announcement-description').text.strip()\n",
    "            except:\n",
    "                key_list[22] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            csv_list.append(dict(zip(header, key_list)))\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error {i}: {str(e)[:50]}\")\n",
    "            continue\n",
    "    \n",
    "    filename = f\"Unegui_bayanzurkh_{date.today().strftime('%Y%m%d')}.csv\"\n",
    "    with codecs.open(filename, 'w', 'utf-8-sig') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_list)\n",
    "    \n",
    "    df = pd.DataFrame(csv_list)\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Done! Scraped {successful}/{len(listing_urls)} listings\")\n",
    "    print(f\"Saved to: {filename}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "    \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da359fdd-5308-44bd-8c07-893782e617bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     –®–∞–ª:      –¢–∞–≥—Ç:   –ì–∞—Ä–∞–∂:  –¶–æ–Ω—Ö: –•–∞–∞–ª–≥–∞: –¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:      –ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü  \\\n",
      "0  –ü–∞—Ä–∫–µ—Ç   1—Ç–∞–≥—Ç—Ç–∞–π   –ë–∞–π–≥–∞–∞  –í–∞–∫—É–º   –¢”©–º”©—Ä          3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "1  –ü–∞—Ä–∫–µ—Ç   2—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          2  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "2  –ü–∞—Ä–∫–µ—Ç  3+—Ç–∞–≥—Ç—Ç–∞–π   –ë–∞–π–≥–∞–∞  –í–∞–∫—É–º   –¢”©–º”©—Ä          8  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "3  –ü–∞—Ä–∫–µ—Ç   1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º     –ú–æ–¥          2  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "4  –ü–∞—Ä–∫–µ—Ç   2—Ç–∞–≥—Ç—Ç–∞–π   –ë–∞–π–≥–∞–∞  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "\n",
      "  –ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω: –ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:  –¢–∞–ª–±–∞–π:  ... –î“Ø“Ø—Ä—ç–≥:  \\\n",
      "0                2019              16  74.15–º¬≤  ...      –£–ë   \n",
      "1                2005               6   49.7–º¬≤  ...      –£–ë   \n",
      "2                2007              13  356.0–º¬≤  ...      –£–ë   \n",
      "3                2013               5   32.0–º¬≤  ...      –£–ë   \n",
      "4                2022              13  76.77–º¬≤  ...      –£–ë   \n",
      "\n",
      "                       –ë–∞–π—Ä—à–∏–ª:       “Æ–∑—Å—ç–Ω: Scraped_date:  \\\n",
      "0  –ë–∞—è–Ω–∑“Ø—Ä—Ö, –ë–∞—è–Ω–∑“Ø—Ä—Ö, –•–æ—Ä–æ–æ 26   “Æ–∑—Å—ç–Ω : 48    2025/11/12   \n",
      "1          –ë–∞—è–Ω–∑“Ø—Ä—Ö, –ó“Ø“Ø–Ω 4 –∑–∞–º  “Æ–∑—Å—ç–Ω : 326    2025/11/12   \n",
      "2   –ë–∞—è–Ω–∑“Ø—Ä—Ö, –ë–∞—è–Ω–∑“Ø—Ä—Ö, –•–æ—Ä–æ–æ 1   “Æ–∑—Å—ç–Ω : 34    2025/11/12   \n",
      "3   –ë–∞—è–Ω–∑“Ø—Ä—Ö, –ë–∞—è–Ω–∑“Ø—Ä—Ö, –•–æ—Ä–æ–æ 5  “Æ–∑—Å—ç–Ω : 214    2025/11/12   \n",
      "4       –ë–∞—è–Ω–∑“Ø—Ä—Ö, –ù–∞—Ä–∞–Ω—Ç—É—É–ª –∑–∞—Ö  “Æ–∑—Å—ç–Ω : 424    2025/11/12   \n",
      "\n",
      "               Posted_date:  \\\n",
      "0  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 20:26   \n",
      "1  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 20:18   \n",
      "2  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 10:48   \n",
      "3  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 09:42   \n",
      "4  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 20:21   \n",
      "\n",
      "                                               Link:           “Æ–Ω—ç:  \\\n",
      "0  https://www.unegui.mn/adv/9856584_bzd-26-r-kho...   408000000.00   \n",
      "1  https://www.unegui.mn/adv/9835263_bzd-zuun-4-z...   230000000.00   \n",
      "2  https://www.unegui.mn/adv/9890552_bzd-1-r-khor...  1500000000.00   \n",
      "3  https://www.unegui.mn/adv/9865730_bzd-negdsen-...   139000000.00   \n",
      "4  https://www.unegui.mn/adv/9565132_bzd-ogoomor-...   262000000.00   \n",
      "\n",
      "  ”®—Ä”©”©–Ω–∏–π –¢–æ–æ:                                      –ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:  \\\n",
      "0       3 ”©—Ä”©”©  –ë–∑–¥ 26-—Ä —Ö–æ—Ä–æ–æ —ç–ª–∏–∑–∞–±–µ—Ç 74.15–º2 –º–∞—Å—Ç–µ—Ä—Ç–∞–π 3 ”©—Ä...   \n",
      "1       2 ”©—Ä”©”©  Bzd zuun 4 zam torgonii zamtai bairni ard 49m2...   \n",
      "2      5+ ”©—Ä”©”©         –ë–∑–¥ 1-—Ä —Ö–æ—Ä–æ–æ royal castel 10 ”©—Ä”©”© –¥“Ø–ø–ª–µ–∫—Å   \n",
      "3       2 ”©—Ä”©”©  –ë–ó–î-–∏–π–Ω –Ω—ç–≥–¥—Å—ç–Ω —ç–º–Ω—ç–ª—ç–≥–∏–π–Ω –±–∞—Ä—É—É–Ω —Ç–∞–ª–¥ 32,37 –º...   \n",
      "4       3 ”©—Ä”©”©  –ë–∑–¥, ”©–≥”©”©–º”©—Ä –∑–∞—Ö—ã–Ω —Ö–æ–π–Ω–æ –Ω–∞—Ä—Ç —Ö–æ—Ç—Ö–æ–Ω–¥ 3 ”©—Ä”©”© 7...   \n",
      "\n",
      "                                      –ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:  \n",
      "0  üè¢ –ë–ó–î 26-—Ä —Ö–æ—Ä–æ–æ –≠–ª–∏–∑–∞–±–µ—Ç –º–∞—Å—Ç–µ—Ä—Ç–∞–π 3 ”©—Ä”©”© \\n ...  \n",
      "1  üå≥ –ë–ó–î –∑“Ø“Ø–Ω 4 –∑–∞–º–¥ 49.7 –º–∫–≤ 2 ”©—Ä”©”© —Ö—É–¥–∞–ª–¥–∞–Ω–∞ \\n...  \n",
      "2  –ë–ó–î ùóò-ùó†ùóÆùóøùòÅ-–Ω —Ö–æ–∏ÃÜ–Ω–æ ùó•ùóºùòÜùóÆùóπ ùóñùóÆùòÄùòÅùóπùó≤ ùü≠ùü¨ ”©—Ä”©”© –¥“Ø–ø–ª–µ...  \n",
      "3  –ë–∑–¥-–∏–π–Ω –Ω—ç–≥–¥—Å—ç–Ω —ç–º–Ω—ç–ª—ç–≥–∏–π–Ω –±–∞—Ä—É—É–Ω —Ç–∞–ª–¥ 32,37–º....  \n",
      "4  –ë–∑–¥ 42-—Ä —Ö–æ—Ä–æ–æ ”©–≥”©”©–º”©—Ä –∑–∞—Ö—ã–Ω \\n–•–æ–π–Ω–æ –Ω–∞—Ä—Ç —Ö–æ—Ç—Ö...  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "header = ['–®–∞–ª:', '–¢–∞–≥—Ç:', '–ì–∞—Ä–∞–∂:', '–¶–æ–Ω—Ö:', '–•–∞–∞–ª–≥–∞:', '–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', '–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', \n",
    "          '–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', '–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', '–¢–∞–ª–±–∞–π:', '–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', \n",
    "          '–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', '–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', '–î“Ø“Ø—Ä—ç–≥:', '–ë–∞–π—Ä—à–∏–ª:', '“Æ–∑—Å—ç–Ω:', \n",
    "          'Scraped_date:', 'Posted_date:', 'Link:', '“Æ–Ω—ç:', '”®—Ä”©”©–Ω–∏–π –¢–æ–æ:', \n",
    "          '–ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:', '–ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:']\n",
    "\n",
    "csv_list = []\n",
    "\n",
    "def key_finder(key, index, span_list, key_list):\n",
    "    for item in span_list:\n",
    "        if str(key) in item:\n",
    "            key_list[index] = item.split(':')[1]\n",
    "    if type(key_list[int(index)]) != str:\n",
    "        key_list[int(index)] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "def extract_listing_links(soup):\n",
    "    listing_links = set()\n",
    "    pattern = re.compile(r'^/adv/\\d+_[a-z0-9-]+/?$')\n",
    "    \n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        if pattern.match(href):\n",
    "            if not any(exclude in href for exclude in ['?', '#', 'page=', 'sort=', 'view=']):\n",
    "                listing_links.add(href)\n",
    "    \n",
    "    return listing_links\n",
    "\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "driver = uc.Chrome(options=options, version_main=None) \n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "try:\n",
    "    base_url = 'https://www.unegui.mn/l-hdlh/l-hdlh-zarna/oron-suuts-zarna/ub-bayanzrh/'   \n",
    "    \n",
    "    driver.get(base_url)\n",
    "    time.sleep(5)  \n",
    "    \n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "    \n",
    "    all_listing_urls = set()\n",
    "    current_page = 1\n",
    "    max_pages = 2  \n",
    "    no_new_count = 0\n",
    "    \n",
    "    while current_page <= max_pages:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        page_listings = extract_listing_links(soup)\n",
    "        \n",
    "        new_listings = page_listings - all_listing_urls\n",
    "        \n",
    "        if len(new_listings) == 0:\n",
    "            no_new_count += 1\n",
    "            if no_new_count >= 2:\n",
    "                break\n",
    "        else:\n",
    "            no_new_count = 0\n",
    "            all_listing_urls.update(new_listings)\n",
    "        \n",
    "        # to next page\n",
    "        if current_page < max_pages:\n",
    "            next_page_url = f\"{base_url}?page={current_page + 1}\"\n",
    "            driver.get(next_page_url)\n",
    "            time.sleep(4)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            current_page += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    listing_urls = [f\"https://www.unegui.mn{link}\" for link in all_listing_urls]\n",
    "    \n",
    "    if len(listing_urls) == 0:\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    \n",
    "    # scraping\n",
    "    start_time = time.time()\n",
    "    successful = 0\n",
    "    \n",
    "    for i, listing_url in enumerate(listing_urls, 1):\n",
    "        try:\n",
    "            driver.get(listing_url)\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            key_list = [\"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"] * len(header)\n",
    "            \n",
    "            try:\n",
    "                details = soup.find('div', class_='announcement-characteristics clearfix').find_all('li')\n",
    "                span_list = [item.text.replace('\\n', '').replace(' ', '') for item in details]\n",
    "            except:\n",
    "                span_list = []\n",
    "            \n",
    "            key_finder('–®–∞–ª:', 0, span_list, key_list) #floor\n",
    "            key_finder('–¢–∞–≥—Ç:', 1, span_list, key_list) #balcony\n",
    "            key_finder('–ì–∞—Ä–∞–∂:', 2, span_list, key_list) #garage\n",
    "            key_finder('–¶–æ–Ω—Ö:', 3, span_list, key_list) #window\n",
    "            key_finder('–•–∞–∞–ª–≥–∞:', 4, span_list, key_list) #door\n",
    "            key_finder('–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', 5, span_list, key_list) #number of windows\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', 6, span_list, key_list) #construction progress\n",
    "            key_finder('–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', 7, span_list, key_list) #built year\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', 8, span_list, key_list) #total floor\n",
    "            key_finder('–¢–∞–ª–±–∞–π:', 9, span_list, key_list) #size\n",
    "            key_finder('–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', 10, span_list, key_list) #located floor\n",
    "            key_finder('–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', 11, span_list, key_list) #elevator\n",
    "            key_finder('–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', 12, span_list, key_list) #payment term\n",
    "            \n",
    "            try:\n",
    "                address = soup.find('span', itemprop=\"address\").text.split('‚Äî')\n",
    "                key_list[13] = address[0].strip() if len(address) > 0 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "                key_list[14] = address[1].strip() if len(address) > 1 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            except:\n",
    "                key_list[13] = key_list[14] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            try:\n",
    "                key_list[15] = soup.find('span', class_='counter-views').text.strip()\n",
    "            except:\n",
    "                key_list[15] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            key_list[16] = date.today().strftime(\"%Y/%m/%d\")\n",
    "            \n",
    "            try:\n",
    "                key_list[17] = soup.find('span', class_='date-meta').text.strip()\n",
    "            except:\n",
    "                key_list[17] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            key_list[18] = listing_url\n",
    "            \n",
    "            try:\n",
    "                key_list[19] = soup.find('meta', itemprop='price')['content']\n",
    "            except:\n",
    "                key_list[19] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            try:\n",
    "                key_list[20] = soup.find('div', class_='wrap js-single-item__location').find_all('span')[-1].text.strip()\n",
    "            except:\n",
    "                key_list[20] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            try:\n",
    "                key_list[21] = soup.find('h1', class_='title-announcement').text.strip()\n",
    "            except:\n",
    "                key_list[21] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            try:\n",
    "                key_list[22] = soup.find('div', class_='announcement-description').text.strip()\n",
    "            except:\n",
    "                key_list[22] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            csv_list.append(dict(zip(header, key_list)))\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    filename = f\"Unegui_bayanzurkh_{date.today().strftime('%Y%m%d')}.csv\"\n",
    "    with codecs.open(filename, 'w', 'utf-8-sig') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_list)\n",
    "    \n",
    "    df = pd.DataFrame(csv_list)\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    pass\n",
    "    \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f955ca27-27b1-457d-a787-b0f17669c64c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
