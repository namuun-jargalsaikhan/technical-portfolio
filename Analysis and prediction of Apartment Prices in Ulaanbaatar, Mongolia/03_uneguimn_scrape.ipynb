{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6107b914-08d4-411d-8f5c-70e382ffdc7a",
   "metadata": {},
   "source": [
    "# Unegui.mn Apartment Scraper\n",
    "\n",
    "Collects apartment ads from Unegui.mn \n",
    "Unegui.mn is the Mongolia's most widely used advertisement site.\n",
    "\n",
    "## How it works\n",
    "\n",
    "1. Opens the website - Uses undetected Chrome to avoid bot detection\n",
    "2. Collects ad links - Scrolls through search results and collects all apartment ad links\n",
    "3. Visits each ad - Goes through each ad one by one and extracts data\n",
    "4. Exports all collected data to one csv file.\n",
    "\n",
    "## Final collected features\n",
    "- Floor type\n",
    "- Balcony\n",
    "- Garage\n",
    "- Window type\n",
    "- Door type\n",
    "- Number of windows\n",
    "- Construction progress\n",
    "- Built year\n",
    "- Total floor\n",
    "- Size\n",
    "- Located floor\n",
    "- Elevator\n",
    "- Payment term\n",
    "- District\n",
    "- Location\n",
    "- View count\n",
    "- Scraped date\n",
    "- Posted date\n",
    "- Ad link\n",
    "- Price\n",
    "- Number of rooms\n",
    "- Ad title\n",
    "- Ad description\n",
    "\n",
    "Scraped data from 8 districts of Ulaanbaatar:\n",
    "- Bayanzurkh \n",
    "- Sukhbaatar \n",
    "- Bayangol \n",
    "- Chingeltei \n",
    "- Khan Uul \n",
    "- Songinokhairkhan \n",
    "- Baganuur\n",
    "- Nalaikh\n",
    "\n",
    "Combined all district datasets into one final dataset for apartment price prediction modeling. This provides geographic diversity and prevents location bias in the prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7239fd78-8e38-4213-8ad1-2f10832cad2b",
   "metadata": {},
   "source": [
    "# Bayanzurkh district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da359fdd-5308-44bd-8c07-893782e617bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     –®–∞–ª:      –¢–∞–≥—Ç:   –ì–∞—Ä–∞–∂:  –¶–æ–Ω—Ö: –•–∞–∞–ª–≥–∞: –¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:      –ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü  \\\n",
      "0  –ü–∞—Ä–∫–µ—Ç   1—Ç–∞–≥—Ç—Ç–∞–π   –ë–∞–π–≥–∞–∞  –í–∞–∫—É–º   –¢”©–º”©—Ä          3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "1  –ü–∞—Ä–∫–µ—Ç   2—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          2  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "2  –ü–∞—Ä–∫–µ—Ç  3+—Ç–∞–≥—Ç—Ç–∞–π   –ë–∞–π–≥–∞–∞  –í–∞–∫—É–º   –¢”©–º”©—Ä          8  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "3  –ü–∞—Ä–∫–µ—Ç   1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º     –ú–æ–¥          2  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "4  –ü–∞—Ä–∫–µ—Ç   2—Ç–∞–≥—Ç—Ç–∞–π   –ë–∞–π–≥–∞–∞  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "\n",
      "  –ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω: –ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:  –¢–∞–ª–±–∞–π:  ... –î“Ø“Ø—Ä—ç–≥:  \\\n",
      "0                2019              16  74.15–º¬≤  ...      –£–ë   \n",
      "1                2005               6   49.7–º¬≤  ...      –£–ë   \n",
      "2                2007              13  356.0–º¬≤  ...      –£–ë   \n",
      "3                2013               5   32.0–º¬≤  ...      –£–ë   \n",
      "4                2022              13  76.77–º¬≤  ...      –£–ë   \n",
      "\n",
      "                       –ë–∞–π—Ä—à–∏–ª:       “Æ–∑—Å—ç–Ω: Scraped_date:  \\\n",
      "0  –ë–∞—è–Ω–∑“Ø—Ä—Ö, –ë–∞—è–Ω–∑“Ø—Ä—Ö, –•–æ—Ä–æ–æ 26   “Æ–∑—Å—ç–Ω : 48    2025/11/12   \n",
      "1          –ë–∞—è–Ω–∑“Ø—Ä—Ö, –ó“Ø“Ø–Ω 4 –∑–∞–º  “Æ–∑—Å—ç–Ω : 326    2025/11/12   \n",
      "2   –ë–∞—è–Ω–∑“Ø—Ä—Ö, –ë–∞—è–Ω–∑“Ø—Ä—Ö, –•–æ—Ä–æ–æ 1   “Æ–∑—Å—ç–Ω : 34    2025/11/12   \n",
      "3   –ë–∞—è–Ω–∑“Ø—Ä—Ö, –ë–∞—è–Ω–∑“Ø—Ä—Ö, –•–æ—Ä–æ–æ 5  “Æ–∑—Å—ç–Ω : 214    2025/11/12   \n",
      "4       –ë–∞—è–Ω–∑“Ø—Ä—Ö, –ù–∞—Ä–∞–Ω—Ç—É—É–ª –∑–∞—Ö  “Æ–∑—Å—ç–Ω : 424    2025/11/12   \n",
      "\n",
      "               Posted_date:  \\\n",
      "0  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 20:26   \n",
      "1  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 20:18   \n",
      "2  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 10:48   \n",
      "3  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 09:42   \n",
      "4  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 20:21   \n",
      "\n",
      "                                               Link:           “Æ–Ω—ç:  \\\n",
      "0  https://www.unegui.mn/adv/9856584_bzd-26-r-kho...   408000000.00   \n",
      "1  https://www.unegui.mn/adv/9835263_bzd-zuun-4-z...   230000000.00   \n",
      "2  https://www.unegui.mn/adv/9890552_bzd-1-r-khor...  1500000000.00   \n",
      "3  https://www.unegui.mn/adv/9865730_bzd-negdsen-...   139000000.00   \n",
      "4  https://www.unegui.mn/adv/9565132_bzd-ogoomor-...   262000000.00   \n",
      "\n",
      "  ”®—Ä”©”©–Ω–∏–π –¢–æ–æ:                                      –ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:  \\\n",
      "0       3 ”©—Ä”©”©  –ë–∑–¥ 26-—Ä —Ö–æ—Ä–æ–æ —ç–ª–∏–∑–∞–±–µ—Ç 74.15–º2 –º–∞—Å—Ç–µ—Ä—Ç–∞–π 3 ”©—Ä...   \n",
      "1       2 ”©—Ä”©”©  Bzd zuun 4 zam torgonii zamtai bairni ard 49m2...   \n",
      "2      5+ ”©—Ä”©”©         –ë–∑–¥ 1-—Ä —Ö–æ—Ä–æ–æ royal castel 10 ”©—Ä”©”© –¥“Ø–ø–ª–µ–∫—Å   \n",
      "3       2 ”©—Ä”©”©  –ë–ó–î-–∏–π–Ω –Ω—ç–≥–¥—Å—ç–Ω —ç–º–Ω—ç–ª—ç–≥–∏–π–Ω –±–∞—Ä—É—É–Ω —Ç–∞–ª–¥ 32,37 –º...   \n",
      "4       3 ”©—Ä”©”©  –ë–∑–¥, ”©–≥”©”©–º”©—Ä –∑–∞—Ö—ã–Ω —Ö–æ–π–Ω–æ –Ω–∞—Ä—Ç —Ö–æ—Ç—Ö–æ–Ω–¥ 3 ”©—Ä”©”© 7...   \n",
      "\n",
      "                                      –ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:  \n",
      "0  üè¢ –ë–ó–î 26-—Ä —Ö–æ—Ä–æ–æ –≠–ª–∏–∑–∞–±–µ—Ç –º–∞—Å—Ç–µ—Ä—Ç–∞–π 3 ”©—Ä”©”© \\n ...  \n",
      "1  üå≥ –ë–ó–î –∑“Ø“Ø–Ω 4 –∑–∞–º–¥ 49.7 –º–∫–≤ 2 ”©—Ä”©”© —Ö—É–¥–∞–ª–¥–∞–Ω–∞ \\n...  \n",
      "2  –ë–ó–î ùóò-ùó†ùóÆùóøùòÅ-–Ω —Ö–æ–∏ÃÜ–Ω–æ ùó•ùóºùòÜùóÆùóπ ùóñùóÆùòÄùòÅùóπùó≤ ùü≠ùü¨ ”©—Ä”©”© –¥“Ø–ø–ª–µ...  \n",
      "3  –ë–∑–¥-–∏–π–Ω –Ω—ç–≥–¥—Å—ç–Ω —ç–º–Ω—ç–ª—ç–≥–∏–π–Ω –±–∞—Ä—É—É–Ω —Ç–∞–ª–¥ 32,37–º....  \n",
      "4  –ë–∑–¥ 42-—Ä —Ö–æ—Ä–æ–æ ”©–≥”©”©–º”©—Ä –∑–∞—Ö—ã–Ω \\n–•–æ–π–Ω–æ –Ω–∞—Ä—Ç —Ö–æ—Ç—Ö...  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "# column names such as floor type, balcony, garage, window type etc\n",
    "header = ['–®–∞–ª:', '–¢–∞–≥—Ç:', '–ì–∞—Ä–∞–∂:', '–¶–æ–Ω—Ö:', '–•–∞–∞–ª–≥–∞:', '–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', '–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', \n",
    "          '–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', '–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', '–¢–∞–ª–±–∞–π:', '–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', \n",
    "          '–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', '–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', '–î“Ø“Ø—Ä—ç–≥:', '–ë–∞–π—Ä—à–∏–ª:', '“Æ–∑—Å—ç–Ω:', \n",
    "          'Scraped_date:', 'Posted_date:', 'Link:', '“Æ–Ω—ç:', '”®—Ä”©”©–Ω–∏–π –¢–æ–æ:', \n",
    "          '–ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:', '–ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:']\n",
    "\n",
    "csv_list = []\n",
    "\n",
    "# searches for specific property attribute in the list and extract the value\n",
    "def key_finder(key, index, span_list, key_list):\n",
    "    for item in span_list:\n",
    "        if str(key) in item:\n",
    "            key_list[index] = item.split(':')[1]\n",
    "    if type(key_list[int(index)]) != str:\n",
    "        key_list[int(index)] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "# extract all unique property ad links from the page\n",
    "def extract_listing_links(soup):\n",
    "    listing_links = set()\n",
    "    pattern = re.compile(r'^/adv/\\d+_[a-z0-9-]+/?$')\n",
    "    \n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        if pattern.match(href):\n",
    "            if not any(exclude in href for exclude in ['?', '#', 'page=', 'sort=', 'view=']):\n",
    "                listing_links.add(href)\n",
    "    \n",
    "    return listing_links\n",
    "\n",
    "# configuring Chrome options\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# undetected Chrome driver for avoiding bot\n",
    "driver = uc.Chrome(options=options, version_main=None) \n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "try:\n",
    "    base_url = 'https://www.unegui.mn/l-hdlh/l-hdlh-zarna/oron-suuts-zarna/ub-bayanzrh/'   \n",
    "\n",
    "    # navigate to base url\n",
    "    driver.get(base_url)\n",
    "    time.sleep(5)  \n",
    "    \n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # pagination variables\n",
    "    all_listing_urls = set()\n",
    "    current_page = 1\n",
    "    max_pages = 2  \n",
    "    no_new_count = 0\n",
    "\n",
    "    # loop through pages and collect ad links\n",
    "    while current_page <= max_pages:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        page_listings = extract_listing_links(soup)\n",
    "        \n",
    "        new_listings = page_listings - all_listing_urls\n",
    "\n",
    "        # stop if no new ad on 2 pages\n",
    "        if len(new_listings) == 0:\n",
    "            no_new_count += 1\n",
    "            if no_new_count >= 2:\n",
    "                break\n",
    "        else:\n",
    "            no_new_count = 0\n",
    "            all_listing_urls.update(new_listings)\n",
    "        \n",
    "        # to next page\n",
    "        if current_page < max_pages:\n",
    "            next_page_url = f\"{base_url}?page={current_page + 1}\"\n",
    "            driver.get(next_page_url)\n",
    "            time.sleep(4)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            current_page += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # converting links\n",
    "    \n",
    "    listing_urls = [f\"https://www.unegui.mn{link}\" for link in all_listing_urls]\n",
    "    \n",
    "    if len(listing_urls) == 0:\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    \n",
    "    # scraping each ad\n",
    "    start_time = time.time()\n",
    "    successful = 0\n",
    "\n",
    "    #going through each and collect data\n",
    "    for i, listing_url in enumerate(listing_urls, 1):\n",
    "        try:\n",
    "            # go to each ad\n",
    "            driver.get(listing_url)\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "            # parsing page html\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            key_list = [\"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"] * len(header)\n",
    "            \n",
    "            try:\n",
    "                details = soup.find('div', class_='announcement-characteristics clearfix').find_all('li')\n",
    "                span_list = [item.text.replace('\\n', '').replace(' ', '') for item in details]\n",
    "            except:\n",
    "                span_list = []\n",
    "\n",
    "            # extracting specific datas\n",
    "            key_finder('–®–∞–ª:', 0, span_list, key_list) #floor\n",
    "            key_finder('–¢–∞–≥—Ç:', 1, span_list, key_list) #balcony\n",
    "            key_finder('–ì–∞—Ä–∞–∂:', 2, span_list, key_list) #garage\n",
    "            key_finder('–¶–æ–Ω—Ö:', 3, span_list, key_list) #window\n",
    "            key_finder('–•–∞–∞–ª–≥–∞:', 4, span_list, key_list) #door\n",
    "            key_finder('–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', 5, span_list, key_list) #number of windows\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', 6, span_list, key_list) #construction progress\n",
    "            key_finder('–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', 7, span_list, key_list) #built year\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', 8, span_list, key_list) #total floor\n",
    "            key_finder('–¢–∞–ª–±–∞–π:', 9, span_list, key_list) #size\n",
    "            key_finder('–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', 10, span_list, key_list) #located floor\n",
    "            key_finder('–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', 11, span_list, key_list) #elevator\n",
    "            key_finder('–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', 12, span_list, key_list) #payment term\n",
    "\n",
    "            # get address\n",
    "            try:\n",
    "                address = soup.find('span', itemprop=\"address\").text.split('‚Äî')\n",
    "                key_list[13] = address[0].strip() if len(address) > 0 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "                key_list[14] = address[1].strip() if len(address) > 1 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            except:\n",
    "                key_list[13] = key_list[14] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # get view count\n",
    "            try:\n",
    "                key_list[15] = soup.find('span', class_='counter-views').text.strip()\n",
    "            except:\n",
    "                key_list[15] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            # scraped date as today\n",
    "            key_list[16] = date.today().strftime(\"%Y/%m/%d\")\n",
    "\n",
    "            # posted date\n",
    "            try:\n",
    "                key_list[17] = soup.find('span', class_='date-meta').text.strip()\n",
    "            except:\n",
    "                key_list[17] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad link\n",
    "            key_list[18] = listing_url\n",
    "\n",
    "            # price\n",
    "            try:\n",
    "                key_list[19] = soup.find('meta', itemprop='price')['content']\n",
    "            except:\n",
    "                key_list[19] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # number of room\n",
    "            try:\n",
    "                key_list[20] = soup.find('div', class_='wrap js-single-item__location').find_all('span')[-1].text.strip()\n",
    "            except:\n",
    "                key_list[20] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad title\n",
    "            try:\n",
    "                key_list[21] = soup.find('h1', class_='title-announcement').text.strip()\n",
    "            except:\n",
    "                key_list[21] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad description\n",
    "            try:\n",
    "                key_list[22] = soup.find('div', class_='announcement-description').text.strip()\n",
    "            except:\n",
    "                key_list[22] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # saving datas to list\n",
    "            csv_list.append(dict(zip(header, key_list)))\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # save the data to csv\n",
    "    filename = f\"Unegui_bayanzurkh_{date.today().strftime('%Y%m%d')}.csv\"\n",
    "    with codecs.open(filename, 'w', 'utf-8-sig') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_list)\n",
    "    \n",
    "    df = pd.DataFrame(csv_list)\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    pass\n",
    "    \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee76ecd4-6526-47d3-af42-f45fae8bce5f",
   "metadata": {},
   "source": [
    "# Sukhbaatar district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5649cffb-fd8a-47b8-8cfe-73009b2aa36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     –®–∞–ª:     –¢–∞–≥—Ç:   –ì–∞—Ä–∞–∂:       –¶–æ–Ω—Ö: –•–∞–∞–ª–≥–∞: –¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:  \\\n",
      "0  –ü–∞—Ä–∫–µ—Ç   –¢–∞–≥—Ç–≥“Ø–π  –ë–∞–π—Ö–≥“Ø–π  –¢”©–º”©—Ä–≤–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          8   \n",
      "1  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π       –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          4   \n",
      "2  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π       –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          2   \n",
      "3  –ü–∞—Ä–∫–µ—Ç  2—Ç–∞–≥—Ç—Ç–∞–π   –ë–∞–π–≥–∞–∞       –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          6   \n",
      "4  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π       –í–∞–∫—É–º   –¢”©–º”©—Ä          3   \n",
      "\n",
      "         –ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü –ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω: –ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:   –¢–∞–ª–±–∞–π:  ...  \\\n",
      "0  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä–æ–æ–≥“Ø–π                2025              10  316.97–º¬≤  ...   \n",
      "1    –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω                2007               9    77.2–º¬≤  ...   \n",
      "2    –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω                2008               9    40.4–º¬≤  ...   \n",
      "3    –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω                2023              16   147.6–º¬≤  ...   \n",
      "4    –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω                2025              16    85.0–º¬≤  ...   \n",
      "\n",
      "  –î“Ø“Ø—Ä—ç–≥:                        –ë–∞–π—Ä—à–∏–ª:       “Æ–∑—Å—ç–Ω: Scraped_date:  \\\n",
      "0      –£–ë                 –°“Ø—Ö–±–∞–∞—Ç–∞—Ä, –¶–∏—Ä–∫   “Æ–∑—Å—ç–Ω : 78    2025/11/12   \n",
      "1      –£–ë   –°“Ø—Ö–±–∞–∞—Ç–∞—Ä, –°“Ø—Ö–±–∞–∞—Ç–∞—Ä, –•–æ—Ä–æ–æ 6   “Æ–∑—Å—ç–Ω : 67    2025/11/12   \n",
      "2      –£–ë   –°“Ø—Ö–±–∞–∞—Ç–∞—Ä, –°“Ø—Ö–±–∞–∞—Ç–∞—Ä, –•–æ—Ä–æ–æ 6   “Æ–∑—Å—ç–Ω : 64    2025/11/12   \n",
      "3      –£–ë                –°“Ø—Ö–±–∞–∞—Ç–∞—Ä, –ú–£–ë–ò–°  “Æ–∑—Å—ç–Ω : 278    2025/11/12   \n",
      "4      –£–ë  –°“Ø—Ö–±–∞–∞—Ç–∞—Ä, –°“Ø—Ö–±–∞–∞—Ç–∞—Ä, –•–æ—Ä–æ–æ 11    “Æ–∑—Å—ç–Ω : 1    2025/11/12   \n",
      "\n",
      "                  Posted_date:  \\\n",
      "0  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-11-09 11:16   \n",
      "1     –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 10:37   \n",
      "2     –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 10:35   \n",
      "3  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-11-10 10:48   \n",
      "4     –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®–Ω”©”©–¥”©—Ä 12:18   \n",
      "\n",
      "                                               Link:           “Æ–Ω—ç:  \\\n",
      "0  https://www.unegui.mn/adv/9759074_tsirkiin-urd...  2535760000.00   \n",
      "1  https://www.unegui.mn/adv/9891042_khunsnii-4-n...   355000000.00   \n",
      "2  https://www.unegui.mn/adv/9891728_novotel-metr...   196000000.00   \n",
      "3  https://www.unegui.mn/adv/9323146_khotyn-tovd-...     7300000.00   \n",
      "4  https://www.unegui.mn/adv/9881122_dolgoon-nuur...     4200000.00   \n",
      "\n",
      "  ”®—Ä”©”©–Ω–∏–π –¢–æ–æ:                                      –ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:  \\\n",
      "0      5+ ”©—Ä”©”©  –¶–∏—Ä–∫–∏–π–Ω —É—Ä–¥ –ºega centro luxury-–¥ 6 ”©—Ä”©”© 316.97...   \n",
      "1       3 ”©—Ä”©”©            –•“Ø–Ω—Å–Ω–∏–π-4, novotel-–Ω –∞—Ä–¥ 3 ”©—Ä”©”© 77.2–º–∫–≤   \n",
      "2       1 ”©—Ä”©”©  Novotel, metro mall-—ã–Ω –∞—Ä–¥ –Ω–æ–≥–æ–æ–Ω —É—Ä–ª–∞–Ω —Ö–æ—Ç—Ö–æ–Ω...   \n",
      "3      5+ ”©—Ä”©”©           –•–æ—Ç—ã–Ω —Ç”©–≤, star tower-—Ç 5 ”©—Ä”©”© 147.6 –º–∫–≤   \n",
      "4       3 ”©—Ä”©”©              –î”©–ª–≥”©”©–Ω –Ω—É—É—Ä—Ç –º–∞—Å—Ç–µ—Ä—Ç–∞–π 85 –º–∫–≤ 3 ”©—Ä”©”©   \n",
      "\n",
      "                                      –ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:  \n",
      "0  üî•–°–ë–î 3-—Ä —Ö–æ—Ä–æ–æ, –ê–°–ê —Ü–∏—Ä–∫–∏–π–Ω —á–∞–Ω—Ö —É—Ä–¥, –±–∞–π—Ä–ª–∞—Ö ...  \n",
      "1  –•–æ—Ç—ã–Ω –ê –±“Ø—Å, Novotel-–Ω –∞—Ä–¥ –æ—Ä—à–∏—Ö –ù–û–ì–û–û–ù –£–†–õ–ê–ù ...  \n",
      "2  –•–æ—Ç—ã–Ω –ê –±“Ø—Å, Novotel-–Ω –∞—Ä–¥ –æ—Ä—à–∏—Ö –ù–û–ì–û–û–ù –£–†–õ–ê–ù ...  \n",
      "3  –•–æ—Ç—ã–Ω —Ç”©–≤–¥ #Star_tower_residence \\n‚òòÔ∏è–ê—Ä—Å–ª–∞–Ω—Ç–∞–π...  \n",
      "4  –°–ë–î, 11 —Ö–æ—Ä–æ–æ, –î”©–ª–≥”©”©–Ω –Ω—É—É—Ä, –°”©“Ø–ª —ç–º–Ω—ç–ª–≥–∏–π–Ω –±–∞...  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "# column names such as floor type, balcony, garage, window type etc\n",
    "header = ['–®–∞–ª:', '–¢–∞–≥—Ç:', '–ì–∞—Ä–∞–∂:', '–¶–æ–Ω—Ö:', '–•–∞–∞–ª–≥–∞:', '–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', '–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', \n",
    "          '–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', '–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', '–¢–∞–ª–±–∞–π:', '–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', \n",
    "          '–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', '–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', '–î“Ø“Ø—Ä—ç–≥:', '–ë–∞–π—Ä—à–∏–ª:', '“Æ–∑—Å—ç–Ω:', \n",
    "          'Scraped_date:', 'Posted_date:', 'Link:', '“Æ–Ω—ç:', '”®—Ä”©”©–Ω–∏–π –¢–æ–æ:', \n",
    "          '–ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:', '–ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:']\n",
    "\n",
    "csv_list = []\n",
    "\n",
    "# searches for specific property attribute in the list and extract the value\n",
    "def key_finder(key, index, span_list, key_list):\n",
    "    for item in span_list:\n",
    "        if str(key) in item:\n",
    "            key_list[index] = item.split(':')[1]\n",
    "    if type(key_list[int(index)]) != str:\n",
    "        key_list[int(index)] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "# extract all unique property ad links from the page\n",
    "def extract_listing_links(soup):\n",
    "    listing_links = set()\n",
    "    pattern = re.compile(r'^/adv/\\d+_[a-z0-9-]+/?$')\n",
    "    \n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        if pattern.match(href):\n",
    "            if not any(exclude in href for exclude in ['?', '#', 'page=', 'sort=', 'view=']):\n",
    "                listing_links.add(href)\n",
    "    \n",
    "    return listing_links\n",
    "\n",
    "# configuring Chrome options\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# undetected Chrome driver for avoiding bot\n",
    "driver = uc.Chrome(options=options, version_main=None) \n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "try:\n",
    "    base_url = 'https://www.unegui.mn/l-hdlh/l-hdlh-zarna/oron-suuts-zarna/ulan-bator/'   \n",
    "\n",
    "    # navigate to base url\n",
    "    driver.get(base_url)\n",
    "    time.sleep(5)  \n",
    "    \n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # pagination variables\n",
    "    all_listing_urls = set()\n",
    "    current_page = 1\n",
    "    max_pages = 2  \n",
    "    no_new_count = 0\n",
    "\n",
    "    # loop through pages and collect ad links\n",
    "    while current_page <= max_pages:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        page_listings = extract_listing_links(soup)\n",
    "        \n",
    "        new_listings = page_listings - all_listing_urls\n",
    "\n",
    "        # stop if no new ad on 2 pages\n",
    "        if len(new_listings) == 0:\n",
    "            no_new_count += 1\n",
    "            if no_new_count >= 2:\n",
    "                break\n",
    "        else:\n",
    "            no_new_count = 0\n",
    "            all_listing_urls.update(new_listings)\n",
    "        \n",
    "        # to next page\n",
    "        if current_page < max_pages:\n",
    "            next_page_url = f\"{base_url}?page={current_page + 1}\"\n",
    "            driver.get(next_page_url)\n",
    "            time.sleep(4)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            current_page += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # converting links\n",
    "    \n",
    "    listing_urls = [f\"https://www.unegui.mn{link}\" for link in all_listing_urls]\n",
    "    \n",
    "    if len(listing_urls) == 0:\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    \n",
    "    # scraping each ad\n",
    "    start_time = time.time()\n",
    "    successful = 0\n",
    "\n",
    "    #going through each and collect data\n",
    "    for i, listing_url in enumerate(listing_urls, 1):\n",
    "        try:\n",
    "            # go to each ad\n",
    "            driver.get(listing_url)\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "            # parsing page html\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            key_list = [\"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"] * len(header)\n",
    "            \n",
    "            try:\n",
    "                details = soup.find('div', class_='announcement-characteristics clearfix').find_all('li')\n",
    "                span_list = [item.text.replace('\\n', '').replace(' ', '') for item in details]\n",
    "            except:\n",
    "                span_list = []\n",
    "\n",
    "            # extracting specific datas\n",
    "            key_finder('–®–∞–ª:', 0, span_list, key_list) #floor\n",
    "            key_finder('–¢–∞–≥—Ç:', 1, span_list, key_list) #balcony\n",
    "            key_finder('–ì–∞—Ä–∞–∂:', 2, span_list, key_list) #garage\n",
    "            key_finder('–¶–æ–Ω—Ö:', 3, span_list, key_list) #window\n",
    "            key_finder('–•–∞–∞–ª–≥–∞:', 4, span_list, key_list) #door\n",
    "            key_finder('–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', 5, span_list, key_list) #number of windows\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', 6, span_list, key_list) #construction progress\n",
    "            key_finder('–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', 7, span_list, key_list) #built year\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', 8, span_list, key_list) #total floor\n",
    "            key_finder('–¢–∞–ª–±–∞–π:', 9, span_list, key_list) #size\n",
    "            key_finder('–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', 10, span_list, key_list) #located floor\n",
    "            key_finder('–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', 11, span_list, key_list) #elevator\n",
    "            key_finder('–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', 12, span_list, key_list) #payment term\n",
    "\n",
    "            # get address\n",
    "            try:\n",
    "                address = soup.find('span', itemprop=\"address\").text.split('‚Äî')\n",
    "                key_list[13] = address[0].strip() if len(address) > 0 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "                key_list[14] = address[1].strip() if len(address) > 1 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            except:\n",
    "                key_list[13] = key_list[14] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # get view count\n",
    "            try:\n",
    "                key_list[15] = soup.find('span', class_='counter-views').text.strip()\n",
    "            except:\n",
    "                key_list[15] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            # scraped date as today\n",
    "            key_list[16] = date.today().strftime(\"%Y/%m/%d\")\n",
    "\n",
    "            # posted date\n",
    "            try:\n",
    "                key_list[17] = soup.find('span', class_='date-meta').text.strip()\n",
    "            except:\n",
    "                key_list[17] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad link\n",
    "            key_list[18] = listing_url\n",
    "\n",
    "            # price\n",
    "            try:\n",
    "                key_list[19] = soup.find('meta', itemprop='price')['content']\n",
    "            except:\n",
    "                key_list[19] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # number of room\n",
    "            try:\n",
    "                key_list[20] = soup.find('div', class_='wrap js-single-item__location').find_all('span')[-1].text.strip()\n",
    "            except:\n",
    "                key_list[20] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad title\n",
    "            try:\n",
    "                key_list[21] = soup.find('h1', class_='title-announcement').text.strip()\n",
    "            except:\n",
    "                key_list[21] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad description\n",
    "            try:\n",
    "                key_list[22] = soup.find('div', class_='announcement-description').text.strip()\n",
    "            except:\n",
    "                key_list[22] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # saving datas to list\n",
    "            csv_list.append(dict(zip(header, key_list)))\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # save the data to csv\n",
    "    filename = f\"Unegui_bayanzurkh_{date.today().strftime('%Y%m%d')}.csv\"\n",
    "    with codecs.open(filename, 'w', 'utf-8-sig') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_list)\n",
    "    \n",
    "    df = pd.DataFrame(csv_list)\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    pass\n",
    "    \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87837555-dd1b-42f2-a92a-8d31e4444c2b",
   "metadata": {},
   "source": [
    "# Bayangol district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b00b438-b3bc-4acd-a2db-884fbca9d52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     –®–∞–ª:     –¢–∞–≥—Ç:   –ì–∞—Ä–∞–∂:       –¶–æ–Ω—Ö:     –•–∞–∞–ª–≥–∞: –¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:  \\\n",
      "0  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π       –í–∞–∫—É–º      –ë“Ø—Ä–≥—ç–¥          3   \n",
      "1  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π       –í–∞–∫—É–º      –ë“Ø—Ä–≥—ç–¥          2   \n",
      "2  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –¢”©–º”©—Ä–≤–∞–∫—É–º  –¢”©–º”©—Ä–≤–∞–∫—É–º          7   \n",
      "3  –ü–∞—Ä–∫–µ—Ç   –¢–∞–≥—Ç–≥“Ø–π  –ë–∞–π—Ö–≥“Ø–π       –í–∞–∫—É–º      –ë“Ø—Ä–≥—ç–¥          2   \n",
      "4  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π       –í–∞–∫—É–º      –ë“Ø—Ä–≥—ç–¥          2   \n",
      "\n",
      "       –ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü –ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω: –ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:  –¢–∞–ª–±–∞–π:  ... –î“Ø“Ø—Ä—ç–≥:  \\\n",
      "0  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω                2006               6   46.0–º¬≤  ...      –£–ë   \n",
      "1  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω                2016              12  55.65–º¬≤  ...      –£–ë   \n",
      "2  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω                2024              16  135.6–º¬≤  ...      –£–ë   \n",
      "3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω                2008               9  42.84–º¬≤  ...      –£–ë   \n",
      "4  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω                2019              16  38.93–º¬≤  ...      –£–ë   \n",
      "\n",
      "                     –ë–∞–π—Ä—à–∏–ª:      “Æ–∑—Å—ç–Ω: Scraped_date:  \\\n",
      "0      –ë–∞—è–Ω–≥–æ–ª, 10-—Ä —Ö–æ—Ä–æ–æ–ª–æ–ª   “Æ–∑—Å—ç–Ω : 5    2025/11/12   \n",
      "1     –ë–∞—è–Ω–≥–æ–ª, –ù–∞—Ä–Ω—ã —Ö–æ—Ä–æ–æ–ª–æ–ª  “Æ–∑—Å—ç–Ω : 21    2025/11/12   \n",
      "2  –ë–∞—è–Ω–≥–æ–ª, –ë–∞—è–Ω–≥–æ–ª, –•–æ—Ä–æ–æ 26  “Æ–∑—Å—ç–Ω : 13    2025/11/12   \n",
      "3          –ë–∞—è–Ω–≥–æ–ª, 6-—Ä –±–∏—á–∏–ª  “Æ–∑—Å—ç–Ω : 54    2025/11/12   \n",
      "4          –ë–∞—è–Ω–≥–æ–ª, –¢”©–º”©—Ä –∑–∞–º  “Æ–∑—Å—ç–Ω : 18    2025/11/12   \n",
      "\n",
      "               Posted_date:  \\\n",
      "0  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®–Ω”©”©–¥”©—Ä 11:58   \n",
      "1  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®–Ω”©”©–¥”©—Ä 11:02   \n",
      "2  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®–Ω”©”©–¥”©—Ä 04:03   \n",
      "3  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 16:30   \n",
      "4  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®–Ω”©”©–¥”©—Ä 11:39   \n",
      "\n",
      "                                               Link:          “Æ–Ω—ç:  \\\n",
      "0  https://www.unegui.mn/adv/9894995_10-khoroolol...  137000000.00   \n",
      "1  https://www.unegui.mn/adv/9894738_eson-erdene-...  255990000.00   \n",
      "2  https://www.unegui.mn/adv/9881412_parko-rivier...  922000000.00   \n",
      "3  https://www.unegui.mn/adv/9889758_khoroolol-bi...  159000000.00   \n",
      "4  https://www.unegui.mn/adv/9886322_tomor-zam-pr...  200000000.00   \n",
      "\n",
      "  ”®—Ä”©”©–Ω–∏–π –¢–æ–æ:                                      –ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:  \\\n",
      "0       2 ”©—Ä”©”©                       10-—Ä —Ö–æ—Ä–æ–æ–ª–æ–ª–¥ 46 –º–∫–≤ 2 ”©—Ä”©”©   \n",
      "1       2 ”©—Ä”©”©       –ï—Å”©–Ω-—ç—Ä–¥—ç–Ω—ç —Ö–æ—Ç—Ö–æ–Ω–¥ 2 ”©—Ä”©”© –æ—Ä–æ–Ω —Å—É—É—Ü 55.65–º2   \n",
      "2       4 ”©—Ä”©”©                  Parko riviera 4 ”©—Ä”©”© 135.6–º–∫ –±–∞–π—Ä   \n",
      "3       1 ”©—Ä”©”©                      –•–æ—Ä–æ–æ–ª–æ–ª –±–∏—á–∏–ª 1 ”©—Ä”©”© 42,84–º2   \n",
      "4       2 ”©—Ä”©”©  –¢”©–º”©—Ä –∑–∞–º, –ø—Ä–µ–º–∏—É–º —É–ª–∞–∞–Ω–±–∞–∞—Ç–∞—Ä —Ö–æ—Ç—Ö–æ–Ω–¥ 38.93 –º...   \n",
      "\n",
      "                                      –ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:  \n",
      "0  –ë–ì–î, 10-—Ä —Ö–æ—Ä–æ–æ–ª–æ–ª –£–ª—Å—ã–Ω 3-—Ä —ç–º–Ω—ç–ª–≥–∏–π–Ω —Ö–æ–π–Ω–æ –≠...  \n",
      "1  –ë–ì–î, 3-—Ä —Ö–æ—Ä–æ–æ #–ï—Å”©–Ω–≠—Ä–¥—ç–Ω—ç —Ö–æ—Ç—Ö–æ–Ω–¥ 55.65–º–∫–≤ 2 ...  \n",
      "2  #Parko riviera -–¥ 135.6–º–∫–≤ #4 ”©—Ä”©”© —Ç–∞–Ω—Å–∞–≥  –±–∞–π...  \n",
      "3  üè† 1 ”©—Ä”©”© –±–∞–π—Ä –∑–∞—Ä–Ω–∞ ‚Äî –ë–∏—á–∏–ª —Ö–æ—Ä–æ–æ–ª–æ–ª\\nüìç –ë–ì–î, 2...  \n",
      "4  –ë–ì–î, Premium ulaanbaatar-–¥ 38.93–º–∫–≤ 2 ”©—Ä”©”© –æ—Ä–æ...  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "# column names such as floor type, balcony, garage, window type etc\n",
    "header = ['–®–∞–ª:', '–¢–∞–≥—Ç:', '–ì–∞—Ä–∞–∂:', '–¶–æ–Ω—Ö:', '–•–∞–∞–ª–≥–∞:', '–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', '–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', \n",
    "          '–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', '–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', '–¢–∞–ª–±–∞–π:', '–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', \n",
    "          '–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', '–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', '–î“Ø“Ø—Ä—ç–≥:', '–ë–∞–π—Ä—à–∏–ª:', '“Æ–∑—Å—ç–Ω:', \n",
    "          'Scraped_date:', 'Posted_date:', 'Link:', '“Æ–Ω—ç:', '”®—Ä”©”©–Ω–∏–π –¢–æ–æ:', \n",
    "          '–ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:', '–ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:']\n",
    "\n",
    "csv_list = []\n",
    "\n",
    "# searches for specific property attribute in the list and extract the value\n",
    "def key_finder(key, index, span_list, key_list):\n",
    "    for item in span_list:\n",
    "        if str(key) in item:\n",
    "            key_list[index] = item.split(':')[1]\n",
    "    if type(key_list[int(index)]) != str:\n",
    "        key_list[int(index)] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "# extract all unique property ad links from the page\n",
    "def extract_listing_links(soup):\n",
    "    listing_links = set()\n",
    "    pattern = re.compile(r'^/adv/\\d+_[a-z0-9-]+/?$')\n",
    "    \n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        if pattern.match(href):\n",
    "            if not any(exclude in href for exclude in ['?', '#', 'page=', 'sort=', 'view=']):\n",
    "                listing_links.add(href)\n",
    "    \n",
    "    return listing_links\n",
    "\n",
    "# configuring Chrome options\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# undetected Chrome driver for avoiding bot\n",
    "driver = uc.Chrome(options=options, version_main=None) \n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "try:\n",
    "    base_url = 'https://www.unegui.mn/l-hdlh/l-hdlh-zarna/oron-suuts-zarna/ub-bayangol/'   \n",
    "\n",
    "    # navigate to base url\n",
    "    driver.get(base_url)\n",
    "    time.sleep(5)  \n",
    "    \n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # pagination variables\n",
    "    all_listing_urls = set()\n",
    "    current_page = 1\n",
    "    max_pages = 2  \n",
    "    no_new_count = 0\n",
    "\n",
    "    # loop through pages and collect ad links\n",
    "    while current_page <= max_pages:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        page_listings = extract_listing_links(soup)\n",
    "        \n",
    "        new_listings = page_listings - all_listing_urls\n",
    "\n",
    "        # stop if no new ad on 2 pages\n",
    "        if len(new_listings) == 0:\n",
    "            no_new_count += 1\n",
    "            if no_new_count >= 2:\n",
    "                break\n",
    "        else:\n",
    "            no_new_count = 0\n",
    "            all_listing_urls.update(new_listings)\n",
    "        \n",
    "        # to next page\n",
    "        if current_page < max_pages:\n",
    "            next_page_url = f\"{base_url}?page={current_page + 1}\"\n",
    "            driver.get(next_page_url)\n",
    "            time.sleep(4)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            current_page += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # converting links\n",
    "    \n",
    "    listing_urls = [f\"https://www.unegui.mn{link}\" for link in all_listing_urls]\n",
    "    \n",
    "    if len(listing_urls) == 0:\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    \n",
    "    # scraping each ad\n",
    "    start_time = time.time()\n",
    "    successful = 0\n",
    "\n",
    "    #going through each and collect data\n",
    "    for i, listing_url in enumerate(listing_urls, 1):\n",
    "        try:\n",
    "            # go to each ad\n",
    "            driver.get(listing_url)\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "            # parsing page html\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            key_list = [\"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"] * len(header)\n",
    "            \n",
    "            try:\n",
    "                details = soup.find('div', class_='announcement-characteristics clearfix').find_all('li')\n",
    "                span_list = [item.text.replace('\\n', '').replace(' ', '') for item in details]\n",
    "            except:\n",
    "                span_list = []\n",
    "\n",
    "            # extracting specific datas\n",
    "            key_finder('–®–∞–ª:', 0, span_list, key_list) #floor\n",
    "            key_finder('–¢–∞–≥—Ç:', 1, span_list, key_list) #balcony\n",
    "            key_finder('–ì–∞—Ä–∞–∂:', 2, span_list, key_list) #garage\n",
    "            key_finder('–¶–æ–Ω—Ö:', 3, span_list, key_list) #window\n",
    "            key_finder('–•–∞–∞–ª–≥–∞:', 4, span_list, key_list) #door\n",
    "            key_finder('–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', 5, span_list, key_list) #number of windows\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', 6, span_list, key_list) #construction progress\n",
    "            key_finder('–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', 7, span_list, key_list) #built year\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', 8, span_list, key_list) #total floor\n",
    "            key_finder('–¢–∞–ª–±–∞–π:', 9, span_list, key_list) #size\n",
    "            key_finder('–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', 10, span_list, key_list) #located floor\n",
    "            key_finder('–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', 11, span_list, key_list) #elevator\n",
    "            key_finder('–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', 12, span_list, key_list) #payment term\n",
    "\n",
    "            # get address\n",
    "            try:\n",
    "                address = soup.find('span', itemprop=\"address\").text.split('‚Äî')\n",
    "                key_list[13] = address[0].strip() if len(address) > 0 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "                key_list[14] = address[1].strip() if len(address) > 1 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            except:\n",
    "                key_list[13] = key_list[14] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # get view count\n",
    "            try:\n",
    "                key_list[15] = soup.find('span', class_='counter-views').text.strip()\n",
    "            except:\n",
    "                key_list[15] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            # scraped date as today\n",
    "            key_list[16] = date.today().strftime(\"%Y/%m/%d\")\n",
    "\n",
    "            # posted date\n",
    "            try:\n",
    "                key_list[17] = soup.find('span', class_='date-meta').text.strip()\n",
    "            except:\n",
    "                key_list[17] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad link\n",
    "            key_list[18] = listing_url\n",
    "\n",
    "            # price\n",
    "            try:\n",
    "                key_list[19] = soup.find('meta', itemprop='price')['content']\n",
    "            except:\n",
    "                key_list[19] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # number of room\n",
    "            try:\n",
    "                key_list[20] = soup.find('div', class_='wrap js-single-item__location').find_all('span')[-1].text.strip()\n",
    "            except:\n",
    "                key_list[20] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad title\n",
    "            try:\n",
    "                key_list[21] = soup.find('h1', class_='title-announcement').text.strip()\n",
    "            except:\n",
    "                key_list[21] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad description\n",
    "            try:\n",
    "                key_list[22] = soup.find('div', class_='announcement-description').text.strip()\n",
    "            except:\n",
    "                key_list[22] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # saving datas to list\n",
    "            csv_list.append(dict(zip(header, key_list)))\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # save the data to csv\n",
    "    filename = f\"Unegui_bayanzurkh_{date.today().strftime('%Y%m%d')}.csv\"\n",
    "    with codecs.open(filename, 'w', 'utf-8-sig') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_list)\n",
    "    \n",
    "    df = pd.DataFrame(csv_list)\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    pass\n",
    "    \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e0fa02-2d28-43bc-bb9d-7cffb751c2a2",
   "metadata": {},
   "source": [
    "# Chingeltei district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dd900a8-7fbf-49ce-a82d-21d83e8ae8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     –®–∞–ª:     –¢–∞–≥—Ç:   –ì–∞—Ä–∞–∂:  –¶–æ–Ω—Ö: –•–∞–∞–ª–≥–∞: –¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:      –ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü  \\\n",
      "0  –¶–µ–º–µ–Ω—Ç  2—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          4  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "1  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          5  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "2  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          5  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "3  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º   –¢”©–º”©—Ä          3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "4  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "\n",
      "  –ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω: –ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:  –¢–∞–ª–±–∞–π:  ... –î“Ø“Ø—Ä—ç–≥:  \\\n",
      "0                2015               3   23.1–º¬≤  ...      –£–ë   \n",
      "1                2000               6  100.0–º¬≤  ...      –£–ë   \n",
      "2                2008              12  122.3–º¬≤  ...      –£–ë   \n",
      "3                2023               9   68.5–º¬≤  ...      –£–ë   \n",
      "4                2008              10   67.0–º¬≤  ...      –£–ë   \n",
      "\n",
      "                         –ë–∞–π—Ä—à–∏–ª:      “Æ–∑—Å—ç–Ω: Scraped_date:  \\\n",
      "0   –ß–∏–Ω–≥—ç–ª—Ç—ç–π, –ß–∏–Ω–≥—ç–ª—Ç—ç–π, –•–æ—Ä–æ–æ 1  “Æ–∑—Å—ç–Ω : 17    2025/11/12   \n",
      "1   –ß–∏–Ω–≥—ç–ª—Ç—ç–π, –ß–∏–Ω–≥—ç–ª—Ç—ç–π, –•–æ—Ä–æ–æ 6  “Æ–∑—Å—ç–Ω : 46    2025/11/12   \n",
      "2           –ß–∏–Ω–≥—ç–ª—Ç—ç–π, –£—Ä—Ç —Ü–∞–≥–∞–∞–Ω   “Æ–∑—Å—ç–Ω : 3    2025/11/12   \n",
      "3  –ß–∏–Ω–≥—ç–ª—Ç—ç–π, –ß–∏–Ω–≥—ç–ª—Ç—ç–π, –•–æ—Ä–æ–æ 10   “Æ–∑—Å—ç–Ω : 8    2025/11/12   \n",
      "4    –ß–∏–Ω–≥—ç–ª—Ç—ç–π, –¢—ç–Ω–≥–∏—Å –∫–∏–Ω–æ —Ç–µ–∞—Ç—Ä  “Æ–∑—Å—ç–Ω : 24    2025/11/12   \n",
      "\n",
      "                  Posted_date:  \\\n",
      "0     –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 16:20   \n",
      "1  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-11-10 13:21   \n",
      "2     –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®–Ω”©”©–¥”©—Ä 09:44   \n",
      "3     –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 10:35   \n",
      "4     –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 16:04   \n",
      "\n",
      "                                               Link:          “Æ–Ω—ç:  \\\n",
      "0  https://www.unegui.mn/adv/9893183_chd-1-r-khor...  545000000.00   \n",
      "1  https://www.unegui.mn/adv/9873772_chd-5r-surgu...  350000000.00   \n",
      "2  https://www.unegui.mn/adv/9832286_ikh-delguur-...  795000000.00   \n",
      "3  https://www.unegui.mn/adv/9809986_shinchlel-kh...  230000000.00   \n",
      "4  https://www.unegui.mn/adv/9893126_tengis-kino-...  266000000.00   \n",
      "\n",
      "  ”®—Ä”©”©–Ω–∏–π –¢–æ–æ:                                      –ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:  \\\n",
      "0       3 ”©—Ä”©”©    –ß–¥ 1-—Ä —Ö–æ—Ä–æ–æ 3 ”©—Ä”©”© –±“Ø—Ä—ç–Ω —Ç–∞–≤–∏–ª–∞–≥–∞—Ç–∞–π –±–∞–π—Ä 23–º2   \n",
      "1       4 ”©—Ä”©”©  –ß–î, 5-—Ä —Å—É—Ä–≥—É—É–ª–∏–π–Ω –±–∞—Ä—É—É–Ω —Ö–æ–π–Ω–æ –≥–∞–ª —Ç–æ–≥–æ–æ —Ç—É—Å–¥...   \n",
      "2       3 ”©—Ä”©”©  –ò—Ö –¥—ç–ª–≥“Ø“Ø—Ä –≥–∞–Ω–¥–∏—Ä—Å –∞–ø–∞—Ä—Ç–º–µ–Ω—Ç–∞–¥ 3 ”©—Ä”©”© 122.3–º–∫ ...   \n",
      "3       2 ”©—Ä”©”©          –®–∏–Ω—ç—á–ª—ç–ª —Ö–æ—Ä–æ–æ–ª–æ–ª 2 ”©—Ä”©”© –æ—Ä–æ–Ω —Å—É—É—Ü 68.5–º2   \n",
      "4       3 ”©—Ä”©”©           –¢—ç–Ω–≥–∏—Å –∫–∏–Ω–æ —Ç–µ–∞—Ç—Ä—ã–Ω —Ö–æ–π–Ω–æ 3 ”©—Ä”©”© 66.6–º–∫–≤   \n",
      "\n",
      "                                      –ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:  \n",
      "0  –•—É–¥–∞–ª–¥–∞–Ω–∞. \\nüìç23-—Ä —Å—É—Ä–≥—É—É–ª–∏–π–Ω —Ö–∞–∂—É—É–¥ \\nüìç–£–ò–î –ß–î...  \n",
      "1  –ë–∞–π—Ä—à–∏–ª: 5-—Ä —Å—É—Ä–≥—É—É–ª–∏–π–Ω –±–∞—Ä—É—É–Ω —Ö–æ–π–Ω–æ (–ï—Ä—Ç”©–Ω—Ü–∏–π...  \n",
      "2  –•–æ—Ç—ã–Ω —Ç”©–≤, –£–ò–î-—ã–Ω –∑“Ø“Ø–Ω —Ç–∞–ª–¥ –ì–∞–Ω–¥–∏—Ä—Å –∞–ø–∞—Ä—Ç–º–µ–Ω—Ç ...  \n",
      "3  üì£–ß–∏–Ω–≥—ç–ª—Ç—ç–π –¥“Ø“Ø—Ä—ç–≥/ –î—ç–Ω–∂–∏–π–Ω –º—è–Ω–≥–∞\\nüìç–®–∏–Ω—ç—á–ª—ç–ª —Ö–æ...  \n",
      "4  üè°–ß–î, 50-—Ä —Å—É—Ä–≥—É—É–ª–∏–π–Ω —Ö–æ–π–Ω–æ, 66.6–º–∫–≤, 3 ”©—Ä”©”© –æ—Ä...  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "# column names such as floor type, balcony, garage, window type etc\n",
    "header = ['–®–∞–ª:', '–¢–∞–≥—Ç:', '–ì–∞—Ä–∞–∂:', '–¶–æ–Ω—Ö:', '–•–∞–∞–ª–≥–∞:', '–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', '–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', \n",
    "          '–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', '–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', '–¢–∞–ª–±–∞–π:', '–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', \n",
    "          '–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', '–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', '–î“Ø“Ø—Ä—ç–≥:', '–ë–∞–π—Ä—à–∏–ª:', '“Æ–∑—Å—ç–Ω:', \n",
    "          'Scraped_date:', 'Posted_date:', 'Link:', '“Æ–Ω—ç:', '”®—Ä”©”©–Ω–∏–π –¢–æ–æ:', \n",
    "          '–ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:', '–ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:']\n",
    "\n",
    "csv_list = []\n",
    "\n",
    "# searches for specific property attribute in the list and extract the value\n",
    "def key_finder(key, index, span_list, key_list):\n",
    "    for item in span_list:\n",
    "        if str(key) in item:\n",
    "            key_list[index] = item.split(':')[1]\n",
    "    if type(key_list[int(index)]) != str:\n",
    "        key_list[int(index)] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "# extract all unique property ad links from the page\n",
    "def extract_listing_links(soup):\n",
    "    listing_links = set()\n",
    "    pattern = re.compile(r'^/adv/\\d+_[a-z0-9-]+/?$')\n",
    "    \n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        if pattern.match(href):\n",
    "            if not any(exclude in href for exclude in ['?', '#', 'page=', 'sort=', 'view=']):\n",
    "                listing_links.add(href)\n",
    "    \n",
    "    return listing_links\n",
    "\n",
    "# configuring Chrome options\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# undetected Chrome driver for avoiding bot\n",
    "driver = uc.Chrome(options=options, version_main=None) \n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "try:\n",
    "    base_url = 'https://www.unegui.mn/l-hdlh/l-hdlh-zarna/oron-suuts-zarna/ub-chingeltej/'   \n",
    "\n",
    "    # navigate to base url\n",
    "    driver.get(base_url)\n",
    "    time.sleep(5)  \n",
    "    \n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # pagination variables\n",
    "    all_listing_urls = set()\n",
    "    current_page = 1\n",
    "    max_pages = 2  \n",
    "    no_new_count = 0\n",
    "\n",
    "    # loop through pages and collect ad links\n",
    "    while current_page <= max_pages:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        page_listings = extract_listing_links(soup)\n",
    "        \n",
    "        new_listings = page_listings - all_listing_urls\n",
    "\n",
    "        # stop if no new ad on 2 pages\n",
    "        if len(new_listings) == 0:\n",
    "            no_new_count += 1\n",
    "            if no_new_count >= 2:\n",
    "                break\n",
    "        else:\n",
    "            no_new_count = 0\n",
    "            all_listing_urls.update(new_listings)\n",
    "        \n",
    "        # to next page\n",
    "        if current_page < max_pages:\n",
    "            next_page_url = f\"{base_url}?page={current_page + 1}\"\n",
    "            driver.get(next_page_url)\n",
    "            time.sleep(4)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            current_page += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # converting links\n",
    "    \n",
    "    listing_urls = [f\"https://www.unegui.mn{link}\" for link in all_listing_urls]\n",
    "    \n",
    "    if len(listing_urls) == 0:\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    \n",
    "    # scraping each ad\n",
    "    start_time = time.time()\n",
    "    successful = 0\n",
    "\n",
    "    #going through each and collect data\n",
    "    for i, listing_url in enumerate(listing_urls, 1):\n",
    "        try:\n",
    "            # go to each ad\n",
    "            driver.get(listing_url)\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "            # parsing page html\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            key_list = [\"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"] * len(header)\n",
    "            \n",
    "            try:\n",
    "                details = soup.find('div', class_='announcement-characteristics clearfix').find_all('li')\n",
    "                span_list = [item.text.replace('\\n', '').replace(' ', '') for item in details]\n",
    "            except:\n",
    "                span_list = []\n",
    "\n",
    "            # extracting specific datas\n",
    "            key_finder('–®–∞–ª:', 0, span_list, key_list) #floor\n",
    "            key_finder('–¢–∞–≥—Ç:', 1, span_list, key_list) #balcony\n",
    "            key_finder('–ì–∞—Ä–∞–∂:', 2, span_list, key_list) #garage\n",
    "            key_finder('–¶–æ–Ω—Ö:', 3, span_list, key_list) #window\n",
    "            key_finder('–•–∞–∞–ª–≥–∞:', 4, span_list, key_list) #door\n",
    "            key_finder('–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', 5, span_list, key_list) #number of windows\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', 6, span_list, key_list) #construction progress\n",
    "            key_finder('–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', 7, span_list, key_list) #built year\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', 8, span_list, key_list) #total floor\n",
    "            key_finder('–¢–∞–ª–±–∞–π:', 9, span_list, key_list) #size\n",
    "            key_finder('–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', 10, span_list, key_list) #located floor\n",
    "            key_finder('–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', 11, span_list, key_list) #elevator\n",
    "            key_finder('–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', 12, span_list, key_list) #payment term\n",
    "\n",
    "            # get address\n",
    "            try:\n",
    "                address = soup.find('span', itemprop=\"address\").text.split('‚Äî')\n",
    "                key_list[13] = address[0].strip() if len(address) > 0 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "                key_list[14] = address[1].strip() if len(address) > 1 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            except:\n",
    "                key_list[13] = key_list[14] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # get view count\n",
    "            try:\n",
    "                key_list[15] = soup.find('span', class_='counter-views').text.strip()\n",
    "            except:\n",
    "                key_list[15] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            # scraped date as today\n",
    "            key_list[16] = date.today().strftime(\"%Y/%m/%d\")\n",
    "\n",
    "            # posted date\n",
    "            try:\n",
    "                key_list[17] = soup.find('span', class_='date-meta').text.strip()\n",
    "            except:\n",
    "                key_list[17] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad link\n",
    "            key_list[18] = listing_url\n",
    "\n",
    "            # price\n",
    "            try:\n",
    "                key_list[19] = soup.find('meta', itemprop='price')['content']\n",
    "            except:\n",
    "                key_list[19] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # number of room\n",
    "            try:\n",
    "                key_list[20] = soup.find('div', class_='wrap js-single-item__location').find_all('span')[-1].text.strip()\n",
    "            except:\n",
    "                key_list[20] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad title\n",
    "            try:\n",
    "                key_list[21] = soup.find('h1', class_='title-announcement').text.strip()\n",
    "            except:\n",
    "                key_list[21] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad description\n",
    "            try:\n",
    "                key_list[22] = soup.find('div', class_='announcement-description').text.strip()\n",
    "            except:\n",
    "                key_list[22] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # saving datas to list\n",
    "            csv_list.append(dict(zip(header, key_list)))\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # save the data to csv\n",
    "    filename = f\"Unegui_bayanzurkh_{date.today().strftime('%Y%m%d')}.csv\"\n",
    "    with codecs.open(filename, 'w', 'utf-8-sig') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_list)\n",
    "    \n",
    "    df = pd.DataFrame(csv_list)\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    pass\n",
    "    \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b2729-cb8b-4bc4-85fc-e5cca337b740",
   "metadata": {},
   "source": [
    "# Khan Uul district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7da7b59-2baf-4b82-a4a4-45b71575b29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      –®–∞–ª:     –¢–∞–≥—Ç:   –ì–∞—Ä–∞–∂:  –¶–æ–Ω—Ö: –•–∞–∞–ª–≥–∞: –¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:      –ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü  \\\n",
      "0   –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          4  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "1   –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º   –¢”©–º”©—Ä          5  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "2  –õ–∞–º–∏–Ω–∞—Ç  1—Ç–∞–≥—Ç—Ç–∞–π   –ë–∞–π–≥–∞–∞  –í–∞–∫—É–º   –¢”©–º”©—Ä          4  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "3   –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "4   –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          6  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "\n",
      "  –ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω: –ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:  –¢–∞–ª–±–∞–π:  ... –î“Ø“Ø—Ä—ç–≥:  \\\n",
      "0                2024              23  109.0–º¬≤  ...      –£–ë   \n",
      "1                2024              25  79.98–º¬≤  ...      –£–ë   \n",
      "2                2024              16  104.0–º¬≤  ...      –£–ë   \n",
      "3                2014              10  69.84–º¬≤  ...      –£–ë   \n",
      "4                2024              23  213.0–º¬≤  ...      –£–ë   \n",
      "\n",
      "                     –ë–∞–π—Ä—à–∏–ª:       “Æ–∑—Å—ç–Ω: Scraped_date:  \\\n",
      "0  –•–∞–Ω-–£—É–ª, –•–∞–Ω-–£—É–ª, –•–æ—Ä–æ–æ 15  “Æ–∑—Å—ç–Ω : 167    2025/11/12   \n",
      "1  –•–∞–Ω-–£—É–ª, –•–∞–Ω-–£—É–ª, –•–æ—Ä–æ–æ 18  “Æ–∑—Å—ç–Ω : 387    2025/11/12   \n",
      "2  –•–∞–Ω-–£—É–ª, –•–∞–Ω-–£—É–ª, –•–æ—Ä–æ–æ 15  “Æ–∑—Å—ç–Ω : 125    2025/11/12   \n",
      "3            –•–∞–Ω-–£—É–ª, UB Town   “Æ–∑—Å—ç–Ω : 49    2025/11/12   \n",
      "4       –•–∞–Ω-–£—É–ª, River Garden  “Æ–∑—Å—ç–Ω : 159    2025/11/12   \n",
      "\n",
      "                  Posted_date:  \\\n",
      "0  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-11-08 12:19   \n",
      "1  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-11-09 15:43   \n",
      "2  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-11-09 07:26   \n",
      "3     –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 21:32   \n",
      "4  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-11-10 19:41   \n",
      "\n",
      "                                               Link:           “Æ–Ω—ç:  \\\n",
      "0  https://www.unegui.mn/adv/9870557_vega-siti-kh...   950000000.00   \n",
      "1  https://www.unegui.mn/adv/9886877_khunnu-2222-...   555000000.00   \n",
      "2  https://www.unegui.mn/adv/9885722_ncd-vega-cit...   910890000.00   \n",
      "3  https://www.unegui.mn/adv/9887365_khud-15-r-kh...   279000000.00   \n",
      "4  https://www.unegui.mn/adv/9605693_river-plaza-...  2002000000.00   \n",
      "\n",
      "  ”®—Ä”©”©–Ω–∏–π –¢–æ–æ:                                      –ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:  \\\n",
      "0       3 ”©—Ä”©”©      –í–µ–≥–∞ —Å–∏—Ç–∏ —Ö–æ—Ç—Ö–æ–Ω–¥ –º–∞—Å—Ç–µ—Ä—Ç–∞–π 3 ”©—Ä”©”© –±–∞–π—Ä 109m2   \n",
      "1       3 ”©—Ä”©”©                    –•“Ø–Ω–Ω“Ø 2222 3-—Ä —ç—ç–ª–∂ 80–º2 3 ”©—Ä”©”©   \n",
      "2       4 ”©—Ä”©”©                      Ncd, vega city 104 –º–∫–≤ 4 ”©—Ä”©”©   \n",
      "3       3 ”©—Ä”©”©  –•—É–¥, 15-—Ä —Ö–æ—Ä–æ–æ 115-—Ä —Å—É—Ä–≥—É—É–ª–∏–π–Ω —Ö–∞–∂—É—É–¥ ub tow...   \n",
      "4      5+ ”©—Ä”©”©   River plaza –æ—Ä–æ–Ω —Å—É—É—Ü–∞–Ω–¥ 213–º–∫–≤ 5 ”©—Ä”©”© –æ—Ä–æ–Ω —Å—É—É—Ü   \n",
      "\n",
      "                                      –ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:  \n",
      "0  –í–µ–≥–∞ —Å–∏—Ç–∏ —Ö–æ—Ç—Ö–æ–Ω–¥ —Å“Ø“Ø–ª–¥ –±–∞—Ä–∏–≥–¥—Å–∞–Ω –±–∞–π—Ä–Ω–∞–∞—Å 109...  \n",
      "1  ‚ÄºÔ∏è–®—É—É–¥ 555 —Å–∞—è–¥ —è–∞—Ä–∞–ª—Ç–∞–π‚ÄºÔ∏è\\n–•“Ø–Ω–Ω“Ø 2222 —Ö–æ—Ç—Ö–æ–Ω—ã...  \n",
      "2  Ncd group, vega city —Ö–æ—Ç—Ö–æ–Ω–¥ 104.7–º–∫–≤ 4 ”©—Ä”©”© –æ...  \n",
      "3  –ë“Ø—Ö —Ü–æ–Ω—Ö —É—Ä–∞–≥—à –≥–æ–ª —Ä—É—É —Ö–∞—Ä—Å–∞–Ω —Ö–∞—Ä—É—É—Ü—Ç–∞–π, –∑–∞—Å–≤–∞...  \n",
      "4  #–•–£–î RIVER PLAZA –±–∏–∑–Ω–µ—Å –∑—ç—Ä—ç–≥–ª—ç–ª–∏–π–Ω –æ—Ä–æ–Ω —Å—É—É—Ü–∞...  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "# column names such as floor type, balcony, garage, window type etc\n",
    "header = ['–®–∞–ª:', '–¢–∞–≥—Ç:', '–ì–∞—Ä–∞–∂:', '–¶–æ–Ω—Ö:', '–•–∞–∞–ª–≥–∞:', '–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', '–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', \n",
    "          '–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', '–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', '–¢–∞–ª–±–∞–π:', '–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', \n",
    "          '–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', '–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', '–î“Ø“Ø—Ä—ç–≥:', '–ë–∞–π—Ä—à–∏–ª:', '“Æ–∑—Å—ç–Ω:', \n",
    "          'Scraped_date:', 'Posted_date:', 'Link:', '“Æ–Ω—ç:', '”®—Ä”©”©–Ω–∏–π –¢–æ–æ:', \n",
    "          '–ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:', '–ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:']\n",
    "\n",
    "csv_list = []\n",
    "\n",
    "# searches for specific property attribute in the list and extract the value\n",
    "def key_finder(key, index, span_list, key_list):\n",
    "    for item in span_list:\n",
    "        if str(key) in item:\n",
    "            key_list[index] = item.split(':')[1]\n",
    "    if type(key_list[int(index)]) != str:\n",
    "        key_list[int(index)] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "# extract all unique property ad links from the page\n",
    "def extract_listing_links(soup):\n",
    "    listing_links = set()\n",
    "    pattern = re.compile(r'^/adv/\\d+_[a-z0-9-]+/?$')\n",
    "    \n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        if pattern.match(href):\n",
    "            if not any(exclude in href for exclude in ['?', '#', 'page=', 'sort=', 'view=']):\n",
    "                listing_links.add(href)\n",
    "    \n",
    "    return listing_links\n",
    "\n",
    "# configuring Chrome options\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# undetected Chrome driver for avoiding bot\n",
    "driver = uc.Chrome(options=options, version_main=None) \n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "try:\n",
    "    base_url = 'https://www.unegui.mn/l-hdlh/l-hdlh-zarna/oron-suuts-zarna/ub-hanuul/'   \n",
    "\n",
    "    # navigate to base url\n",
    "    driver.get(base_url)\n",
    "    time.sleep(5)  \n",
    "    \n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # pagination variables\n",
    "    all_listing_urls = set()\n",
    "    current_page = 1\n",
    "    max_pages = 2  \n",
    "    no_new_count = 0\n",
    "\n",
    "    # loop through pages and collect ad links\n",
    "    while current_page <= max_pages:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        page_listings = extract_listing_links(soup)\n",
    "        \n",
    "        new_listings = page_listings - all_listing_urls\n",
    "\n",
    "        # stop if no new ad on 2 pages\n",
    "        if len(new_listings) == 0:\n",
    "            no_new_count += 1\n",
    "            if no_new_count >= 2:\n",
    "                break\n",
    "        else:\n",
    "            no_new_count = 0\n",
    "            all_listing_urls.update(new_listings)\n",
    "        \n",
    "        # to next page\n",
    "        if current_page < max_pages:\n",
    "            next_page_url = f\"{base_url}?page={current_page + 1}\"\n",
    "            driver.get(next_page_url)\n",
    "            time.sleep(4)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            current_page += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # converting links\n",
    "    \n",
    "    listing_urls = [f\"https://www.unegui.mn{link}\" for link in all_listing_urls]\n",
    "    \n",
    "    if len(listing_urls) == 0:\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    \n",
    "    # scraping each ad\n",
    "    start_time = time.time()\n",
    "    successful = 0\n",
    "\n",
    "    #going through each and collect data\n",
    "    for i, listing_url in enumerate(listing_urls, 1):\n",
    "        try:\n",
    "            # go to each ad\n",
    "            driver.get(listing_url)\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "            # parsing page html\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            key_list = [\"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"] * len(header)\n",
    "            \n",
    "            try:\n",
    "                details = soup.find('div', class_='announcement-characteristics clearfix').find_all('li')\n",
    "                span_list = [item.text.replace('\\n', '').replace(' ', '') for item in details]\n",
    "            except:\n",
    "                span_list = []\n",
    "\n",
    "            # extracting specific datas\n",
    "            key_finder('–®–∞–ª:', 0, span_list, key_list) #floor\n",
    "            key_finder('–¢–∞–≥—Ç:', 1, span_list, key_list) #balcony\n",
    "            key_finder('–ì–∞—Ä–∞–∂:', 2, span_list, key_list) #garage\n",
    "            key_finder('–¶–æ–Ω—Ö:', 3, span_list, key_list) #window\n",
    "            key_finder('–•–∞–∞–ª–≥–∞:', 4, span_list, key_list) #door\n",
    "            key_finder('–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', 5, span_list, key_list) #number of windows\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', 6, span_list, key_list) #construction progress\n",
    "            key_finder('–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', 7, span_list, key_list) #built year\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', 8, span_list, key_list) #total floor\n",
    "            key_finder('–¢–∞–ª–±–∞–π:', 9, span_list, key_list) #size\n",
    "            key_finder('–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', 10, span_list, key_list) #located floor\n",
    "            key_finder('–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', 11, span_list, key_list) #elevator\n",
    "            key_finder('–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', 12, span_list, key_list) #payment term\n",
    "\n",
    "            # get address\n",
    "            try:\n",
    "                address = soup.find('span', itemprop=\"address\").text.split('‚Äî')\n",
    "                key_list[13] = address[0].strip() if len(address) > 0 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "                key_list[14] = address[1].strip() if len(address) > 1 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            except:\n",
    "                key_list[13] = key_list[14] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # get view count\n",
    "            try:\n",
    "                key_list[15] = soup.find('span', class_='counter-views').text.strip()\n",
    "            except:\n",
    "                key_list[15] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            # scraped date as today\n",
    "            key_list[16] = date.today().strftime(\"%Y/%m/%d\")\n",
    "\n",
    "            # posted date\n",
    "            try:\n",
    "                key_list[17] = soup.find('span', class_='date-meta').text.strip()\n",
    "            except:\n",
    "                key_list[17] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad link\n",
    "            key_list[18] = listing_url\n",
    "\n",
    "            # price\n",
    "            try:\n",
    "                key_list[19] = soup.find('meta', itemprop='price')['content']\n",
    "            except:\n",
    "                key_list[19] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # number of room\n",
    "            try:\n",
    "                key_list[20] = soup.find('div', class_='wrap js-single-item__location').find_all('span')[-1].text.strip()\n",
    "            except:\n",
    "                key_list[20] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad title\n",
    "            try:\n",
    "                key_list[21] = soup.find('h1', class_='title-announcement').text.strip()\n",
    "            except:\n",
    "                key_list[21] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad description\n",
    "            try:\n",
    "                key_list[22] = soup.find('div', class_='announcement-description').text.strip()\n",
    "            except:\n",
    "                key_list[22] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # saving datas to list\n",
    "            csv_list.append(dict(zip(header, key_list)))\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # save the data to csv\n",
    "    filename = f\"Unegui_bayanzurkh_{date.today().strftime('%Y%m%d')}.csv\"\n",
    "    with codecs.open(filename, 'w', 'utf-8-sig') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_list)\n",
    "    \n",
    "    df = pd.DataFrame(csv_list)\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    pass\n",
    "    \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c8496-cb10-4dce-b51e-4c888309b61c",
   "metadata": {},
   "source": [
    "# Songinokhairkhan district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85ddcfd3-b196-49cc-b74f-c5663261b483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     –®–∞–ª:     –¢–∞–≥—Ç:   –ì–∞—Ä–∞–∂:  –¶–æ–Ω—Ö:     –•–∞–∞–ª–≥–∞: –¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:      –ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü  \\\n",
      "0  –ü–∞—Ä–∫–µ—Ç   –¢–∞–≥—Ç–≥“Ø–π   –ë–∞–π–≥–∞–∞  –í–∞–∫—É–º      –ë“Ø—Ä–≥—ç–¥          2  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "1  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π   –ë–∞–π–≥–∞–∞  –í–∞–∫—É–º       –¢”©–º”©—Ä          3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "2  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –¢”©–º”©—Ä–≤–∞–∫—É–º          3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "3  –ü–∞—Ä–∫–µ—Ç  2—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º      –ë“Ø—Ä–≥—ç–¥          3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "4  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π   –ë–∞–π–≥–∞–∞  –í–∞–∫—É–º       –¢”©–º”©—Ä          3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "\n",
      "  –ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω: –ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:  –¢–∞–ª–±–∞–π:  ... –î“Ø“Ø—Ä—ç–≥:  \\\n",
      "0                2025               3   57.7–º¬≤  ...      –£–ë   \n",
      "1                2025              15  78.49–º¬≤  ...      –£–ë   \n",
      "2                2025              10  72.03–º¬≤  ...      –£–ë   \n",
      "3                2006               5   86.0–º¬≤  ...      –£–ë   \n",
      "4                2024              16   51.0–º¬≤  ...      –£–ë   \n",
      "\n",
      "                                   –ë–∞–π—Ä—à–∏–ª:      “Æ–∑—Å—ç–Ω: Scraped_date:  \\\n",
      "0             –°–æ–Ω–≥–∏–Ω–æ—Ö–∞–π—Ä—Ö–∞–Ω, 21-—Ä —Ö–æ—Ä–æ–æ–ª–æ–ª   “Æ–∑—Å—ç–Ω : 9    2025/11/12   \n",
      "1  –°–æ–Ω–≥–∏–Ω–æ—Ö–∞–π—Ä—Ö–∞–Ω, –°–æ–Ω–≥–∏–Ω–æ—Ö–∞–π—Ä—Ö–∞–Ω, –•–æ—Ä–æ–æ 13   “Æ–∑—Å—ç–Ω : 0    2025/11/12   \n",
      "2                     –°–æ–Ω–≥–∏–Ω–æ—Ö–∞–π—Ä—Ö–∞–Ω, –û—Ä–±–∏—Ç  “Æ–∑—Å—ç–Ω : 25    2025/11/12   \n",
      "3                     –°–æ–Ω–≥–∏–Ω–æ—Ö–∞–π—Ä—Ö–∞–Ω, 5 —à–∞—Ä   “Æ–∑—Å—ç–Ω : 6    2025/11/12   \n",
      "4                     –°–æ–Ω–≥–∏–Ω–æ—Ö–∞–π—Ä—Ö–∞–Ω, 5 —à–∞—Ä   “Æ–∑—Å—ç–Ω : 4    2025/11/12   \n",
      "\n",
      "               Posted_date:  \\\n",
      "0  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 17:00   \n",
      "1  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®–Ω”©”©–¥”©—Ä 08:42   \n",
      "2  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®–Ω”©”©–¥”©—Ä 09:27   \n",
      "3  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®—á–∏–≥–¥”©—Ä 19:17   \n",
      "4  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: ”®–Ω”©”©–¥”©—Ä 11:30   \n",
      "\n",
      "                                               Link:          “Æ–Ω—ç:  \\\n",
      "0  https://www.unegui.mn/adv/9893360_skhd-chingis...  184640000.00   \n",
      "1  https://www.unegui.mn/adv/9850636_skhd-moskva-...  324500000.00   \n",
      "2  https://www.unegui.mn/adv/9889897_roman-park-k...  194481000.00   \n",
      "3  https://www.unegui.mn/adv/9893167_5-shar-avtob...  280000000.00   \n",
      "4  https://www.unegui.mn/adv/9840577_orgil-21-kho...  216725000.00   \n",
      "\n",
      "  ”®—Ä”©”©–Ω–∏–π –¢–æ–æ:                                      –ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:  \\\n",
      "0       2 ”©—Ä”©”©  –°—Ö–¥ —á–∏–Ω–≥–∏—Å —Å–æ–æ—Å—ç –±–∞—Ä—É—É–Ω —Ç–∞–ª–¥ —á–∞–Ω–¥–º–∞–Ω—å –∞–ø–∞—Ä—Ç–º–µ–Ω...   \n",
      "1       3 ”©—Ä”©”©           –°—Ö–¥, –º–æ—Å–∫–≤–∞ –∞–ø–∞—Ä—Ç–º—ç–Ω—Ç–∞–¥ 78.49 –º–∫–≤ 3 ”©—Ä”©”©   \n",
      "2       3 ”©—Ä”©”©        Roman park —Ö–æ—Ç—Ö–æ–Ω–¥ 3 ”©—Ä”©”© 72.03–º–∫ –æ—Ä–æ–Ω —Å—É—É—Ü   \n",
      "3       4 ”©—Ä”©”©       5 —à–∞—Ä –∞–≤—Ç–æ–±—É—Å–Ω—ã –±—É—É–¥–∞–ª–¥ –æ–π—Ä 4 ”©—Ä”©”© –±–∞–π—Ä 86–º2   \n",
      "4       2 ”©—Ä”©”©          –û—Ä–≥–∏–ª 21 —Ö–æ—Ç—Ö–æ–Ω —ç–Ω–∫–∞–Ω—Ç–æ 51–º–∫–≤ 2 ”©—Ä”©”© –±–∞–π—Ä   \n",
      "\n",
      "                                      –ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:  \n",
      "0  2025 –æ–Ω–¥ –∞—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥ –æ—Ä—Å–æ–Ω 2 ”©—Ä”©”©\\n\\n–ë–∞—Ä—É—É–Ω –±–æ...  \n",
      "1  FOR SALE: –ú–æ—Å–∫–≤–∞ –∞–ø–∞—Ä—Ç–º–µ–Ω—Ç \\nüìç–£–ª–∞–∞–Ω–±–∞–∞—Ç–∞—Ä —Ö–æ—Ç,...  \n",
      "2  ‚òòÔ∏è ROMAN PARK —Ö–æ—Ç—Ö–æ–Ω–¥ —Ü–æ–æ —à–∏–Ω—ç 3 ”©—Ä”©”© –æ—Ä–æ–Ω —Å—É—É...  \n",
      "3  5 —à–∞—Ä –º–æ–Ω–æ—Å —ç–º–∏–π–Ω —Å–∞–Ω—Ç–∞–π –±–∞–π—Ä, 5/4 –¥–∞–≤—Ö–∞—Ä—Ç, 86...  \n",
      "4  -–î—Ä–∞–≥–æ–Ω—ã –∞—Ä–¥, –°“Ø“Ø –•–•–ö —É—Ä–¥ \\n-–û—Ä–≥–∏–ª 21 —Ö–æ—Ç—Ö–æ–Ω \\...  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "# column names such as floor type, balcony, garage, window type etc\n",
    "header = ['–®–∞–ª:', '–¢–∞–≥—Ç:', '–ì–∞—Ä–∞–∂:', '–¶–æ–Ω—Ö:', '–•–∞–∞–ª–≥–∞:', '–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', '–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', \n",
    "          '–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', '–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', '–¢–∞–ª–±–∞–π:', '–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', \n",
    "          '–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', '–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', '–î“Ø“Ø—Ä—ç–≥:', '–ë–∞–π—Ä—à–∏–ª:', '“Æ–∑—Å—ç–Ω:', \n",
    "          'Scraped_date:', 'Posted_date:', 'Link:', '“Æ–Ω—ç:', '”®—Ä”©”©–Ω–∏–π –¢–æ–æ:', \n",
    "          '–ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:', '–ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:']\n",
    "\n",
    "csv_list = []\n",
    "\n",
    "# searches for specific property attribute in the list and extract the value\n",
    "def key_finder(key, index, span_list, key_list):\n",
    "    for item in span_list:\n",
    "        if str(key) in item:\n",
    "            key_list[index] = item.split(':')[1]\n",
    "    if type(key_list[int(index)]) != str:\n",
    "        key_list[int(index)] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "# extract all unique property ad links from the page\n",
    "def extract_listing_links(soup):\n",
    "    listing_links = set()\n",
    "    pattern = re.compile(r'^/adv/\\d+_[a-z0-9-]+/?$')\n",
    "    \n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        if pattern.match(href):\n",
    "            if not any(exclude in href for exclude in ['?', '#', 'page=', 'sort=', 'view=']):\n",
    "                listing_links.add(href)\n",
    "    \n",
    "    return listing_links\n",
    "\n",
    "# configuring Chrome options\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# undetected Chrome driver for avoiding bot\n",
    "driver = uc.Chrome(options=options, version_main=None) \n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "try:\n",
    "    base_url = 'https://www.unegui.mn/l-hdlh/l-hdlh-zarna/oron-suuts-zarna/ub-songinohajrhan/'   \n",
    "\n",
    "    # navigate to base url\n",
    "    driver.get(base_url)\n",
    "    time.sleep(5)  \n",
    "    \n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # pagination variables\n",
    "    all_listing_urls = set()\n",
    "    current_page = 1\n",
    "    max_pages = 2  \n",
    "    no_new_count = 0\n",
    "\n",
    "    # loop through pages and collect ad links\n",
    "    while current_page <= max_pages:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        page_listings = extract_listing_links(soup)\n",
    "        \n",
    "        new_listings = page_listings - all_listing_urls\n",
    "\n",
    "        # stop if no new ad on 2 pages\n",
    "        if len(new_listings) == 0:\n",
    "            no_new_count += 1\n",
    "            if no_new_count >= 2:\n",
    "                break\n",
    "        else:\n",
    "            no_new_count = 0\n",
    "            all_listing_urls.update(new_listings)\n",
    "        \n",
    "        # to next page\n",
    "        if current_page < max_pages:\n",
    "            next_page_url = f\"{base_url}?page={current_page + 1}\"\n",
    "            driver.get(next_page_url)\n",
    "            time.sleep(4)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            current_page += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # converting links\n",
    "    \n",
    "    listing_urls = [f\"https://www.unegui.mn{link}\" for link in all_listing_urls]\n",
    "    \n",
    "    if len(listing_urls) == 0:\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    \n",
    "    # scraping each ad\n",
    "    start_time = time.time()\n",
    "    successful = 0\n",
    "\n",
    "    #going through each and collect data\n",
    "    for i, listing_url in enumerate(listing_urls, 1):\n",
    "        try:\n",
    "            # go to each ad\n",
    "            driver.get(listing_url)\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "            # parsing page html\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            key_list = [\"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"] * len(header)\n",
    "            \n",
    "            try:\n",
    "                details = soup.find('div', class_='announcement-characteristics clearfix').find_all('li')\n",
    "                span_list = [item.text.replace('\\n', '').replace(' ', '') for item in details]\n",
    "            except:\n",
    "                span_list = []\n",
    "\n",
    "            # extracting specific datas\n",
    "            key_finder('–®–∞–ª:', 0, span_list, key_list) #floor\n",
    "            key_finder('–¢–∞–≥—Ç:', 1, span_list, key_list) #balcony\n",
    "            key_finder('–ì–∞—Ä–∞–∂:', 2, span_list, key_list) #garage\n",
    "            key_finder('–¶–æ–Ω—Ö:', 3, span_list, key_list) #window\n",
    "            key_finder('–•–∞–∞–ª–≥–∞:', 4, span_list, key_list) #door\n",
    "            key_finder('–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', 5, span_list, key_list) #number of windows\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', 6, span_list, key_list) #construction progress\n",
    "            key_finder('–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', 7, span_list, key_list) #built year\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', 8, span_list, key_list) #total floor\n",
    "            key_finder('–¢–∞–ª–±–∞–π:', 9, span_list, key_list) #size\n",
    "            key_finder('–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', 10, span_list, key_list) #located floor\n",
    "            key_finder('–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', 11, span_list, key_list) #elevator\n",
    "            key_finder('–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', 12, span_list, key_list) #payment term\n",
    "\n",
    "            # get address\n",
    "            try:\n",
    "                address = soup.find('span', itemprop=\"address\").text.split('‚Äî')\n",
    "                key_list[13] = address[0].strip() if len(address) > 0 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "                key_list[14] = address[1].strip() if len(address) > 1 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            except:\n",
    "                key_list[13] = key_list[14] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # get view count\n",
    "            try:\n",
    "                key_list[15] = soup.find('span', class_='counter-views').text.strip()\n",
    "            except:\n",
    "                key_list[15] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            # scraped date as today\n",
    "            key_list[16] = date.today().strftime(\"%Y/%m/%d\")\n",
    "\n",
    "            # posted date\n",
    "            try:\n",
    "                key_list[17] = soup.find('span', class_='date-meta').text.strip()\n",
    "            except:\n",
    "                key_list[17] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad link\n",
    "            key_list[18] = listing_url\n",
    "\n",
    "            # price\n",
    "            try:\n",
    "                key_list[19] = soup.find('meta', itemprop='price')['content']\n",
    "            except:\n",
    "                key_list[19] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # number of room\n",
    "            try:\n",
    "                key_list[20] = soup.find('div', class_='wrap js-single-item__location').find_all('span')[-1].text.strip()\n",
    "            except:\n",
    "                key_list[20] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad title\n",
    "            try:\n",
    "                key_list[21] = soup.find('h1', class_='title-announcement').text.strip()\n",
    "            except:\n",
    "                key_list[21] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad description\n",
    "            try:\n",
    "                key_list[22] = soup.find('div', class_='announcement-description').text.strip()\n",
    "            except:\n",
    "                key_list[22] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # saving datas to list\n",
    "            csv_list.append(dict(zip(header, key_list)))\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # save the data to csv\n",
    "    filename = f\"Unegui_bayanzurkh_{date.today().strftime('%Y%m%d')}.csv\"\n",
    "    with codecs.open(filename, 'w', 'utf-8-sig') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_list)\n",
    "    \n",
    "    df = pd.DataFrame(csv_list)\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    pass\n",
    "    \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f16eea-7a7f-4a4e-a470-2121708c1768",
   "metadata": {},
   "source": [
    "# Baganuur district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9feecc09-92d9-428c-8d0d-323d57a6ad09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     –®–∞–ª:     –¢–∞–≥—Ç:   –ì–∞—Ä–∞–∂:  –¶–æ–Ω—Ö: –•–∞–∞–ª–≥–∞: –¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:      –ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü  \\\n",
      "0  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          2  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "1  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º   –¢”©–º”©—Ä          3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "2  –ü–∞—Ä–∫–µ—Ç  2—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "3  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          2  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "\n",
      "  –ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω: –ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä: –¢–∞–ª–±–∞–π:  ... –î“Ø“Ø—Ä—ç–≥:  \\\n",
      "0                1980               5  36.0–º¬≤  ...      –£–ë   \n",
      "1                1986               5  47.0–º¬≤  ...      –£–ë   \n",
      "2                2024               9  55.3–º¬≤  ...      –£–ë   \n",
      "3                1985               5  34.0–º¬≤  ...      –£–ë   \n",
      "\n",
      "                      –ë–∞–π—Ä—à–∏–ª:      “Æ–∑—Å—ç–Ω: Scraped_date:  \\\n",
      "0  –ë–∞–≥–∞–Ω—É—É—Ä, –ë–∞–≥–∞–Ω—É—É—Ä, –•–æ—Ä–æ–æ 1  “Æ–∑—Å—ç–Ω : 44    2025/11/12   \n",
      "1  –ë–∞–≥–∞–Ω—É—É—Ä, –ë–∞–≥–∞–Ω—É—É—Ä, –•–æ—Ä–æ–æ 1  “Æ–∑—Å—ç–Ω : 78    2025/11/12   \n",
      "2  –ë–∞–≥–∞–Ω—É—É—Ä, –ë–∞–≥–∞–Ω—É—É—Ä, –•–æ—Ä–æ–æ 3  “Æ–∑—Å—ç–Ω : 55    2025/11/12   \n",
      "3  –ë–∞–≥–∞–Ω—É—É—Ä, –ë–∞–≥–∞–Ω—É—É—Ä, –•–æ—Ä–æ–æ 1  “Æ–∑—Å—ç–Ω : 47    2025/11/12   \n",
      "\n",
      "                  Posted_date:  \\\n",
      "0  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-11-07 10:46   \n",
      "1  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-10-19 11:22   \n",
      "2  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-10-13 22:08   \n",
      "3  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-10-29 09:58   \n",
      "\n",
      "                                               Link:          “Æ–Ω—ç:  \\\n",
      "0  https://www.unegui.mn/adv/9880839_baga-nuuryn-...   95000000.00   \n",
      "1  https://www.unegui.mn/adv/9826215_baganuurt-2-...  140000000.00   \n",
      "2  https://www.unegui.mn/adv/9811444_baganuur-duu...  170000000.00   \n",
      "3  https://www.unegui.mn/adv/9751358_baganuurt-1-...   92340000.00   \n",
      "\n",
      "  ”®—Ä”©”©–Ω–∏–π –¢–æ–æ:                                      –ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:  \\\n",
      "0       1 ”©—Ä”©”©                        –ë–∞–≥–∞ –Ω—É—É—Ä—Ç 1 ”©—Ä”©”© 36–º–∫ –±–∞–π—Ä   \n",
      "1       2 ”©—Ä”©”©                         –ë–∞–≥–∞–Ω—É—É—Ä—Ç 2 ”©—Ä”©”© –±–∞–π—Ä 47–º2   \n",
      "2       2 ”©—Ä”©”©  –ë–∞–≥–∞–Ω—É—É—Ä –¥“Ø“Ø—Ä—ç–≥—Ç, —Ü–∞—Ö–∏–ª–≥–∞–∞–Ω —Ç“Ø–≥—ç—ç—Ö–∏–π–Ω –±–∞–π—Ä–∞–Ω–¥ ...   \n",
      "3       1 ”©—Ä”©”©                            –ë–∞–≥–∞–Ω—É—É—Ä—Ç 34 –º–∫–≤ 1 ”©—Ä”©”©   \n",
      "\n",
      "                                      –ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:  \n",
      "0  –ë–∞–≥–∞ –Ω—É—É—Ä—Ç 1 ”©—Ä”©”© –±–∞–π—Ä—ã–≥ —Ö–æ—Ç–æ–¥ —Ö/–±–∞–π—à–∏–Ω —ç—Å 1-2...  \n",
      "1  üè† 2 ”®–†”®”® –ë–ê–ô–† –•–£–î–ê–õ–î–ê–ù–ê\\nüìç –ë–∞–≥–∞–Ω—É—É—Ä –¥“Ø“Ø—Ä—ç–≥, 1-...  \n",
      "2  2024 –æ–Ω–¥ –∞—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥ –æ—Ä—Å–æ–Ω. 6-–Ω —Å–∞—Ä–∞–∞—Å —Ö–æ–π—à –∞–º...  \n",
      "3  –ë–∞–≥–∞–Ω—É—É—Ä –¥“Ø“Ø—Ä—ç–≥—Ç –≥–∞–ª —Ç–æ–≥–æ–æ —Ç—É—Å–¥–∞–∞ 1 ”©—Ä”©”© –±–∞–π—Ä ...  \n",
      "\n",
      "[4 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "# column names such as floor type, balcony, garage, window type etc\n",
    "header = ['–®–∞–ª:', '–¢–∞–≥—Ç:', '–ì–∞—Ä–∞–∂:', '–¶–æ–Ω—Ö:', '–•–∞–∞–ª–≥–∞:', '–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', '–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', \n",
    "          '–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', '–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', '–¢–∞–ª–±–∞–π:', '–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', \n",
    "          '–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', '–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', '–î“Ø“Ø—Ä—ç–≥:', '–ë–∞–π—Ä—à–∏–ª:', '“Æ–∑—Å—ç–Ω:', \n",
    "          'Scraped_date:', 'Posted_date:', 'Link:', '“Æ–Ω—ç:', '”®—Ä”©”©–Ω–∏–π –¢–æ–æ:', \n",
    "          '–ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:', '–ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:']\n",
    "\n",
    "csv_list = []\n",
    "\n",
    "# searches for specific property attribute in the list and extract the value\n",
    "def key_finder(key, index, span_list, key_list):\n",
    "    for item in span_list:\n",
    "        if str(key) in item:\n",
    "            key_list[index] = item.split(':')[1]\n",
    "    if type(key_list[int(index)]) != str:\n",
    "        key_list[int(index)] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "# extract all unique property ad links from the page\n",
    "def extract_listing_links(soup):\n",
    "    listing_links = set()\n",
    "    pattern = re.compile(r'^/adv/\\d+_[a-z0-9-]+/?$')\n",
    "    \n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        if pattern.match(href):\n",
    "            if not any(exclude in href for exclude in ['?', '#', 'page=', 'sort=', 'view=']):\n",
    "                listing_links.add(href)\n",
    "    \n",
    "    return listing_links\n",
    "\n",
    "# configuring Chrome options\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# undetected Chrome driver for avoiding bot\n",
    "driver = uc.Chrome(options=options, version_main=None) \n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "try:\n",
    "    base_url = 'https://www.unegui.mn/l-hdlh/l-hdlh-zarna/oron-suuts-zarna/ub-baganuur/'   \n",
    "\n",
    "    # navigate to base url\n",
    "    driver.get(base_url)\n",
    "    time.sleep(5)  \n",
    "    \n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # pagination variables\n",
    "    all_listing_urls = set()\n",
    "    current_page = 1\n",
    "    max_pages = 2  \n",
    "    no_new_count = 0\n",
    "\n",
    "    # loop through pages and collect ad links\n",
    "    while current_page <= max_pages:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        page_listings = extract_listing_links(soup)\n",
    "        \n",
    "        new_listings = page_listings - all_listing_urls\n",
    "\n",
    "        # stop if no new ad on 2 pages\n",
    "        if len(new_listings) == 0:\n",
    "            no_new_count += 1\n",
    "            if no_new_count >= 2:\n",
    "                break\n",
    "        else:\n",
    "            no_new_count = 0\n",
    "            all_listing_urls.update(new_listings)\n",
    "        \n",
    "        # to next page\n",
    "        if current_page < max_pages:\n",
    "            next_page_url = f\"{base_url}?page={current_page + 1}\"\n",
    "            driver.get(next_page_url)\n",
    "            time.sleep(4)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            current_page += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # converting links\n",
    "    \n",
    "    listing_urls = [f\"https://www.unegui.mn{link}\" for link in all_listing_urls]\n",
    "    \n",
    "    if len(listing_urls) == 0:\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    \n",
    "    # scraping each ad\n",
    "    start_time = time.time()\n",
    "    successful = 0\n",
    "\n",
    "    #going through each and collect data\n",
    "    for i, listing_url in enumerate(listing_urls, 1):\n",
    "        try:\n",
    "            # go to each ad\n",
    "            driver.get(listing_url)\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "            # parsing page html\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            key_list = [\"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"] * len(header)\n",
    "            \n",
    "            try:\n",
    "                details = soup.find('div', class_='announcement-characteristics clearfix').find_all('li')\n",
    "                span_list = [item.text.replace('\\n', '').replace(' ', '') for item in details]\n",
    "            except:\n",
    "                span_list = []\n",
    "\n",
    "            # extracting specific datas\n",
    "            key_finder('–®–∞–ª:', 0, span_list, key_list) #floor\n",
    "            key_finder('–¢–∞–≥—Ç:', 1, span_list, key_list) #balcony\n",
    "            key_finder('–ì–∞—Ä–∞–∂:', 2, span_list, key_list) #garage\n",
    "            key_finder('–¶–æ–Ω—Ö:', 3, span_list, key_list) #window\n",
    "            key_finder('–•–∞–∞–ª–≥–∞:', 4, span_list, key_list) #door\n",
    "            key_finder('–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', 5, span_list, key_list) #number of windows\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', 6, span_list, key_list) #construction progress\n",
    "            key_finder('–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', 7, span_list, key_list) #built year\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', 8, span_list, key_list) #total floor\n",
    "            key_finder('–¢–∞–ª–±–∞–π:', 9, span_list, key_list) #size\n",
    "            key_finder('–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', 10, span_list, key_list) #located floor\n",
    "            key_finder('–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', 11, span_list, key_list) #elevator\n",
    "            key_finder('–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', 12, span_list, key_list) #payment term\n",
    "\n",
    "            # get address\n",
    "            try:\n",
    "                address = soup.find('span', itemprop=\"address\").text.split('‚Äî')\n",
    "                key_list[13] = address[0].strip() if len(address) > 0 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "                key_list[14] = address[1].strip() if len(address) > 1 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            except:\n",
    "                key_list[13] = key_list[14] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # get view count\n",
    "            try:\n",
    "                key_list[15] = soup.find('span', class_='counter-views').text.strip()\n",
    "            except:\n",
    "                key_list[15] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            # scraped date as today\n",
    "            key_list[16] = date.today().strftime(\"%Y/%m/%d\")\n",
    "\n",
    "            # posted date\n",
    "            try:\n",
    "                key_list[17] = soup.find('span', class_='date-meta').text.strip()\n",
    "            except:\n",
    "                key_list[17] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad link\n",
    "            key_list[18] = listing_url\n",
    "\n",
    "            # price\n",
    "            try:\n",
    "                key_list[19] = soup.find('meta', itemprop='price')['content']\n",
    "            except:\n",
    "                key_list[19] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # number of room\n",
    "            try:\n",
    "                key_list[20] = soup.find('div', class_='wrap js-single-item__location').find_all('span')[-1].text.strip()\n",
    "            except:\n",
    "                key_list[20] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad title\n",
    "            try:\n",
    "                key_list[21] = soup.find('h1', class_='title-announcement').text.strip()\n",
    "            except:\n",
    "                key_list[21] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad description\n",
    "            try:\n",
    "                key_list[22] = soup.find('div', class_='announcement-description').text.strip()\n",
    "            except:\n",
    "                key_list[22] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # saving datas to list\n",
    "            csv_list.append(dict(zip(header, key_list)))\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # save the data to csv\n",
    "    filename = f\"Unegui_bayanzurkh_{date.today().strftime('%Y%m%d')}.csv\"\n",
    "    with codecs.open(filename, 'w', 'utf-8-sig') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_list)\n",
    "    \n",
    "    df = pd.DataFrame(csv_list)\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    pass\n",
    "    \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f32e1bb-f8c4-442a-9f4e-b14c3db15819",
   "metadata": {},
   "source": [
    "# Nalaikh district"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "875e33a2-6be2-463b-9e0f-adfb0cf7a1ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     –®–∞–ª:     –¢–∞–≥—Ç:   –ì–∞—Ä–∞–∂:  –¶–æ–Ω—Ö: –•–∞–∞–ª–≥–∞: –¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:      –ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü  \\\n",
      "0  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          2  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "1  –ü–∞—Ä–∫–µ—Ç  2—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          5  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "2  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          4  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "3  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          4  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "4  –ü–∞—Ä–∫–µ—Ç  1—Ç–∞–≥—Ç—Ç–∞–π  –ë–∞–π—Ö–≥“Ø–π  –í–∞–∫—É–º  –ë“Ø—Ä–≥—ç–¥          3  –ê—à–∏–≥–ª–∞–ª—Ç–∞–¥–æ—Ä—Å–æ–Ω   \n",
      "\n",
      "  –ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω: –ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:  –¢–∞–ª–±–∞–π:  ... –î“Ø“Ø—Ä—ç–≥:  \\\n",
      "0                2000               5   32.0–º¬≤  ...      –£–ë   \n",
      "1                2014               5  77.56–º¬≤  ...      –£–ë   \n",
      "2                2000               5   68.0–º¬≤  ...      –£–ë   \n",
      "3                2000               5   55.3–º¬≤  ...      –£–ë   \n",
      "4                2015               5  45.21–º¬≤  ...      –£–ë   \n",
      "\n",
      "                  –ë–∞–π—Ä—à–∏–ª:       “Æ–∑—Å—ç–Ω: Scraped_date:  \\\n",
      "0  –ù–∞–ª–∞–π—Ö, –ù–∞–ª–∞–π—Ö, –•–æ—Ä–æ–æ 5   “Æ–∑—Å—ç–Ω : 80    2025/11/12   \n",
      "1  –ù–∞–ª–∞–π—Ö, –ù–∞–ª–∞–π—Ö, –•–æ—Ä–æ–æ 7   “Æ–∑—Å—ç–Ω : 47    2025/11/12   \n",
      "2  –ù–∞–ª–∞–π—Ö, –ù–∞–ª–∞–π—Ö, –•–æ—Ä–æ–æ 2  “Æ–∑—Å—ç–Ω : 168    2025/11/12   \n",
      "3  –ù–∞–ª–∞–π—Ö, –ù–∞–ª–∞–π—Ö, –•–æ—Ä–æ–æ 5   “Æ–∑—Å—ç–Ω : 48    2025/11/12   \n",
      "4  –ù–∞–ª–∞–π—Ö, –ù–∞–ª–∞–π—Ö, –•–æ—Ä–æ–æ 5   “Æ–∑—Å—ç–Ω : 58    2025/11/12   \n",
      "\n",
      "                  Posted_date:  \\\n",
      "0  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-11-10 11:22   \n",
      "1  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-10-20 11:14   \n",
      "2  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-11-02 18:53   \n",
      "3  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-11-06 22:59   \n",
      "4  –ù–∏–π—Ç—ç–ª—Å—ç–Ω: 2025-10-26 13:45   \n",
      "\n",
      "                                               Link:          “Æ–Ω—ç:  \\\n",
      "0  https://www.unegui.mn/adv/9582183_gordokt-1-or...   65000000.00   \n",
      "1  https://www.unegui.mn/adv/9753610_bzd-3-r-khor...  178000000.00   \n",
      "2  https://www.unegui.mn/adv/9867121_nalaikhad-bu...  180000000.00   \n",
      "3  https://www.unegui.mn/adv/9880362_nalaikh-duur...   85000000.00   \n",
      "4  https://www.unegui.mn/adv/9828048_nalaikh-goro...  110000000.00   \n",
      "\n",
      "  ”®—Ä”©”©–Ω–∏–π –¢–æ–æ:                                      –ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:  \\\n",
      "0       1 ”©—Ä”©”©                         –ì–æ—Ä–¥–æ–∫—Ç 1 ”©—Ä”©”© 32 –º–∫–≤ –±–∞–π—Ä   \n",
      "1       3 ”©—Ä”©”©       –ù–∞–ª–∞–π—Ö –¥“Ø“Ø—Ä–≥–∏–π–Ω –∞–Ω—É —Ö–æ—Ç—Ö–æ–Ω–¥ 77.56 –º–∫–≤ 3 ”©—Ä”©”©   \n",
      "2       3 ”©—Ä”©”©                             –ù–∞–ª–∞–π—Ö–∞–¥ 3 ”©—Ä”©”© 68 –º–∫–≤   \n",
      "3       2 ”©—Ä”©”©  –ù–∞–ª–∞–π—Ö –¥“Ø“Ø—Ä—ç–≥ 5-—Ä —Ö–æ—Ä–æ–æ –≥–æ—Ä–æ–¥–æ–∫—Ç 2 ”©—Ä”©”© 55.3–º–∫...   \n",
      "4       2 ”©—Ä”©”©               –ù–∞–ª–∞–π—Ö –≥–æ—Ä–æ–¥–æ–∫—Ç 2 ”©—Ä”©”© –±–∞–π—Ä 45.21–º–∫–≤   \n",
      "\n",
      "                                      –ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:  \n",
      "0  –ú”©–Ω–≥”©–Ω–∏–π —Ö—ç—Ä—ç–≥ –≥–∞—Ä—Å–∞–Ω —Ç—É–ª –∑–∞—Ä–∞—Ö–∞–∞—Ä –±–æ–ª—Å–æ–æ–Ω. \\n...  \n",
      "1  –£–ë —Ö–æ—Ç –ù–∞–ª–∞–π—Ö –¥“Ø“Ø—Ä–≥–∏–π–Ω 8-—Ä —Ö–æ—Ä–æ–æ –ê–ù–£ —Ö–æ—Ç—Ö–æ–Ω–¥ 7...  \n",
      "2  –£—É—Ä—Ö–∞–π—á–¥—ã–Ω 5–¥–∞–≤—Ö–∞—Ä—Ç 3 ”©—Ä”©”© –±–∞–π—Ä —Ö—É–¥–∞–ª–¥–∞–Ω–∞. ”©–≤”©...  \n",
      "3  –ì–æ—Ä–æ–¥–æ–∫—Ç 76-—Ä –±–∞–π—Ä–Ω—ã 4-—Ä –æ—Ä—Ü–æ–Ω–¥ 55.3–º–∫–≤ –≥–∞–ª —Ç–æ...  \n",
      "4  –ù–∞–ª–∞–π—Ö—ã–Ω –≥–æ—Ä–æ–¥–æ–∫ ‚Äú–¶—ç–Ω–≥—ç–≥‚Äù —Ö–æ—Ç—Ö–æ–Ω–¥ –º–∞—à —Ü—ç–≤—ç—Ä—Ö—ç–Ω...  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "import csv\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "import re\n",
    "\n",
    "# column names such as floor type, balcony, garage, window type etc\n",
    "header = ['–®–∞–ª:', '–¢–∞–≥—Ç:', '–ì–∞—Ä–∞–∂:', '–¶–æ–Ω—Ö:', '–•–∞–∞–ª–≥–∞:', '–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', '–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', \n",
    "          '–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', '–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', '–¢–∞–ª–±–∞–π:', '–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', \n",
    "          '–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', '–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', '–î“Ø“Ø—Ä—ç–≥:', '–ë–∞–π—Ä—à–∏–ª:', '“Æ–∑—Å—ç–Ω:', \n",
    "          'Scraped_date:', 'Posted_date:', 'Link:', '“Æ–Ω—ç:', '”®—Ä”©”©–Ω–∏–π –¢–æ–æ:', \n",
    "          '–ó–∞—Ä—ã–Ω –≥–∞—Ä—á–∏–≥:', '–ó–∞—Ä—ã–Ω –¢–∞–π–ª–±–∞—Ä:']\n",
    "\n",
    "csv_list = []\n",
    "\n",
    "# searches for specific property attribute in the list and extract the value\n",
    "def key_finder(key, index, span_list, key_list):\n",
    "    for item in span_list:\n",
    "        if str(key) in item:\n",
    "            key_list[index] = item.split(':')[1]\n",
    "    if type(key_list[int(index)]) != str:\n",
    "        key_list[int(index)] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "# extract all unique property ad links from the page\n",
    "def extract_listing_links(soup):\n",
    "    listing_links = set()\n",
    "    pattern = re.compile(r'^/adv/\\d+_[a-z0-9-]+/?$')\n",
    "    \n",
    "    all_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    for link in all_links:\n",
    "        href = link['href']\n",
    "        if pattern.match(href):\n",
    "            if not any(exclude in href for exclude in ['?', '#', 'page=', 'sort=', 'view=']):\n",
    "                listing_links.add(href)\n",
    "    \n",
    "    return listing_links\n",
    "\n",
    "# configuring Chrome options\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# undetected Chrome driver for avoiding bot\n",
    "driver = uc.Chrome(options=options, version_main=None) \n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "try:\n",
    "    base_url = 'https://www.unegui.mn/l-hdlh/l-hdlh-zarna/oron-suuts-zarna/ub-nalajh/'   \n",
    "\n",
    "    # navigate to base url\n",
    "    driver.get(base_url)\n",
    "    time.sleep(5)  \n",
    "    \n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # pagination variables\n",
    "    all_listing_urls = set()\n",
    "    current_page = 1\n",
    "    max_pages = 2  \n",
    "    no_new_count = 0\n",
    "\n",
    "    # loop through pages and collect ad links\n",
    "    while current_page <= max_pages:\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        page_listings = extract_listing_links(soup)\n",
    "        \n",
    "        new_listings = page_listings - all_listing_urls\n",
    "\n",
    "        # stop if no new ad on 2 pages\n",
    "        if len(new_listings) == 0:\n",
    "            no_new_count += 1\n",
    "            if no_new_count >= 2:\n",
    "                break\n",
    "        else:\n",
    "            no_new_count = 0\n",
    "            all_listing_urls.update(new_listings)\n",
    "        \n",
    "        # to next page\n",
    "        if current_page < max_pages:\n",
    "            next_page_url = f\"{base_url}?page={current_page + 1}\"\n",
    "            driver.get(next_page_url)\n",
    "            time.sleep(4)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href^='/adv/']\")))\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)\n",
    "            \n",
    "            current_page += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # converting links\n",
    "    \n",
    "    listing_urls = [f\"https://www.unegui.mn{link}\" for link in all_listing_urls]\n",
    "    \n",
    "    if len(listing_urls) == 0:\n",
    "        driver.quit()\n",
    "        exit()\n",
    "    \n",
    "    # scraping each ad\n",
    "    start_time = time.time()\n",
    "    successful = 0\n",
    "\n",
    "    #going through each and collect data\n",
    "    for i, listing_url in enumerate(listing_urls, 1):\n",
    "        try:\n",
    "            # go to each ad\n",
    "            driver.get(listing_url)\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "            # parsing page html\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            key_list = [\"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"] * len(header)\n",
    "            \n",
    "            try:\n",
    "                details = soup.find('div', class_='announcement-characteristics clearfix').find_all('li')\n",
    "                span_list = [item.text.replace('\\n', '').replace(' ', '') for item in details]\n",
    "            except:\n",
    "                span_list = []\n",
    "\n",
    "            # extracting specific datas\n",
    "            key_finder('–®–∞–ª:', 0, span_list, key_list) #floor\n",
    "            key_finder('–¢–∞–≥—Ç:', 1, span_list, key_list) #balcony\n",
    "            key_finder('–ì–∞—Ä–∞–∂:', 2, span_list, key_list) #garage\n",
    "            key_finder('–¶–æ–Ω—Ö:', 3, span_list, key_list) #window\n",
    "            key_finder('–•–∞–∞–ª–≥–∞:', 4, span_list, key_list) #door\n",
    "            key_finder('–¶–æ–Ω—Ö–Ω—ã—Ç–æ–æ:', 5, span_list, key_list) #number of windows\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω—è–≤—Ü', 6, span_list, key_list) #construction progress\n",
    "            key_finder('–ê—à–∏–≥–ª–∞–ª—Ç–∞–Ω–¥–æ—Ä—Å–æ–Ω–æ–Ω:', 7, span_list, key_list) #built year\n",
    "            key_finder('–ë–∞—Ä–∏–ª–≥—ã–Ω–¥–∞–≤—Ö–∞—Ä:', 8, span_list, key_list) #total floor\n",
    "            key_finder('–¢–∞–ª–±–∞–π:', 9, span_list, key_list) #size\n",
    "            key_finder('–•—ç–¥—ç–Ω–¥–∞–≤—Ö–∞—Ä—Ç:', 10, span_list, key_list) #located floor\n",
    "            key_finder('–¶–∞—Ö–∏–ª–≥–∞–∞–Ω—à–∞—Ç—Ç–∞–π—ç—Å—ç—Ö:', 11, span_list, key_list) #elevator\n",
    "            key_finder('–¢”©–ª–±”©—Ä–∏–π–Ω–Ω”©—Ö—Ü”©–ª:', 12, span_list, key_list) #payment term\n",
    "\n",
    "            # get address\n",
    "            try:\n",
    "                address = soup.find('span', itemprop=\"address\").text.split('‚Äî')\n",
    "                key_list[13] = address[0].strip() if len(address) > 0 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "                key_list[14] = address[1].strip() if len(address) > 1 else \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            except:\n",
    "                key_list[13] = key_list[14] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # get view count\n",
    "            try:\n",
    "                key_list[15] = soup.find('span', class_='counter-views').text.strip()\n",
    "            except:\n",
    "                key_list[15] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "            \n",
    "            # scraped date as today\n",
    "            key_list[16] = date.today().strftime(\"%Y/%m/%d\")\n",
    "\n",
    "            # posted date\n",
    "            try:\n",
    "                key_list[17] = soup.find('span', class_='date-meta').text.strip()\n",
    "            except:\n",
    "                key_list[17] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad link\n",
    "            key_list[18] = listing_url\n",
    "\n",
    "            # price\n",
    "            try:\n",
    "                key_list[19] = soup.find('meta', itemprop='price')['content']\n",
    "            except:\n",
    "                key_list[19] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # number of room\n",
    "            try:\n",
    "                key_list[20] = soup.find('div', class_='wrap js-single-item__location').find_all('span')[-1].text.strip()\n",
    "            except:\n",
    "                key_list[20] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad title\n",
    "            try:\n",
    "                key_list[21] = soup.find('h1', class_='title-announcement').text.strip()\n",
    "            except:\n",
    "                key_list[21] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # ad description\n",
    "            try:\n",
    "                key_list[22] = soup.find('div', class_='announcement-description').text.strip()\n",
    "            except:\n",
    "                key_list[22] = \"–ú—ç–¥—ç—ç–ª—ç–ª –±–∞–π—Ö–≥“Ø–π\"\n",
    "\n",
    "            # saving datas to list\n",
    "            csv_list.append(dict(zip(header, key_list)))\n",
    "            successful += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    # save the data to csv\n",
    "    filename = f\"Unegui_bayanzurkh_{date.today().strftime('%Y%m%d')}.csv\"\n",
    "    with codecs.open(filename, 'w', 'utf-8-sig') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_list)\n",
    "    \n",
    "    df = pd.DataFrame(csv_list)\n",
    "    print(df.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    pass\n",
    "    \n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7e622-7ef0-4e0a-aa4a-14366cb0d7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
